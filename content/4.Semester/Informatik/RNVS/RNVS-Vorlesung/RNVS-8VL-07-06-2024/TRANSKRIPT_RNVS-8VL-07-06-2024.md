---
tags:
  - 4semester
  - informatik
  - RNVS
fach: "[[Rechnernetze und Verteilte Systeme (RNVS)]]"
Thema:
Benötigte Zeit:
date created: Friday, 7. June 2024, 09:19
date modified: Saturday, 6. July 2024, 01:46
---

Herzlich Willkommen zur Vorlesung Rechner, Netze und verteilte Systeme. Wir sind noch immer im Kapitel 3 über die Transportschicht und das ist Teil 7 und zugleich der letzte Teil dieses Kapitels. Zu diesem Zeitpunkt sollten Sie grundsätzlich wissen, wie die Transportschicht funktioniert und welche Funktionen wir dort haben. In diesem Teil geht es noch etwas um die Vertiefung, das heißt wir wollen uns Spezialitäten anschauen und noch etwas tiefer einsteigen. Wir haben uns ja im letzten Teil über die Erhöhung der Kommunikationsleistung unterhalten, das heißt wie kriegen wir höheren Durchsatz, wie kriegen wir eine bessere Auslastung. Und jetzt wollen wir noch einmal… …einen Blick auf die Zuverlässigkeit werfen und uns anschauen, wie funktionieren Flusskontrolle und Staukontrolle bzw. Überlastkontrolle. Und wir haben schon gesagt, unser Ziel ist es, dass wir eine höhere Leistung bei geringerer Fehlerrate bekommen. Werfen wir mal grundsätzlich einen Blick auf, was ist Flusskontrolle, was ist Staukontrolle. Also bei der Flusskontrolle oder bei der Flusssteuerung, im englischen Flow Control, geht es darum, dass wir den Sender, …so managen, dass der den Empfänger nicht überlastet. Das heißt, das Ziel ist immer, dass der Empfänger nicht mehr bekommt, als er tatsächlich abarbeiten kann. Wir brauchen also irgendein Signal vom Empfänger, ob er denn die nächste Nachricht überhaupt behandeln kann, ob er überhaupt bereit ist, diese Nachricht zu empfangen. Bei der Staukontrolle oder auch Überlastkontrolle geht es darum, dass wir Maßnahmen setzen, um die Überlastung der… …Transit-Netze im Griff zu behalten. Das heißt, wir haben ja gesagt, dass im Internet viele andere gleichzeitig die gleichen Ressourcen nutzen. Und weil die gemeinsame Nutzung einfach vorliegt und ein Grundprinzip ist, müssen wir uns überlegen, was wir tun, wenn die anderen Teilnehmer im Internet mehr von diesen Transit-Netzen in Anspruch nehmen und dadurch unsere eigenen Nachrichten verzögert werden. Wir können dieses… …Verkehrsmanagement, das wir hier sehen, auch auf den täglichen Straßenverkehr umlegen. Und das zeigt dieses Bild hier ganz schön. Also wir sehen hier links die Sender. Das sind jeweils ein paar kleine Einfamilienhäuser. Dort parken rot ein paar Fahrzeuge, Verkehrsteilnehmer. Das sind dann umgelegt auf unser Kommunikationsmodell die Nachrichten, die abgeschickt werden sollen. Und wir sehen ganz rechts mit der Ampel dargestellt das Ziel. Das sind gerade vier Fahrzeuge. Vielleicht die Einfahrt zum Parkplatz, vielleicht das, wo die Universität ist und wo wir im Endeffekt hinwollen und wo wir einen Parkplatz für unser Vehikel benötigen. Dazwischen sehen wir die Transit-Netze. Also in blau eingekastelt. Wir sehen ein paar Straßen. Wir sehen unterschiedliche Wege, die von den Sendestationen zum Ziel führen. Wir sehen auch, dass andere Pakete bereits unterwegs sind. Und zwar sind bei jedem dieser Kreisverkehre praktisch ein paar Fahrzeuge, die bereits jetzt auf die Weiterfahrt warten. Und was wir jetzt machen wollen, ist nichts anderes, als dass wir Nachrichten ins Netz abschicken wollen. Und wir wollen das aber so machen, dass weder das Teil dazwischen, also das Transit-Netz, noch der Parkplatz beim Empfänger überlastet ist. Also wir sehen ja auch, dass die Ampel beim Empfänger auf rot geschaltet ist. Jetzt schauen wir uns einmal an, was passiert, wenn wir einfach einmal die vier Fahrzeuge loslegen. Wir sehen, eines davon kommt gleich in den ersten Kreisverkehr rein. Währenddessen bewegt sich im anderen Kreisverkehr eins weiter. Und so weiter. Die Nachrichten fließen durchs Internet. Und wir haben eigentlich wenig Kontrolle über das, was die anderen machen. Wir können nur kontrollieren, was wir selbst entsprechend einbringen. Und genauso geht es jetzt auch weiter, wenn wir uns den nächsten Zeitritt anschauen. Wir sehen, dass die Ampel beim Parkplatz… Der Parkplatz geht auf grün und das bedeutet, dass jetzt ein Fahrzeug einfahren kann auf den Parkplatz. Also wir haben jetzt dort nur noch drei in der Wartecue vor dem Schranken entsprechend stehen. Und das bedeutet auch, dass wir praktisch im nächsten Schritt ein viertes Fahrzeug dazustellen können, weil jetzt wieder Platz ist. Das heißt, durch den Empfänger wird ein Paket aus dem Verkehrsgewimmel genommen. Und entsprechend auch auf den anderen Knoten etwas raus. Weiter getrieben. Also wieder, wenn wir das jetzt umsetzen auf unser Internetbeispiel, dann sind eben die Kreisverkehre die Route dazwischen, die irgendwo in Transitnetzen sind, die uns vielleicht gar nicht bekannt sind, die aber dennoch eben den Verkehr von mehreren Stationen abbilden müssen. Und das kann man dann auch schön weiterspielen. Das heißt, die nächsten Fahrzeuge gehen rein vor jedem dieser Kreisverkehre, warten Fahrzeuge, bis sie einfahren dürfen. Und irgendwann schaltet auch der… Empfänger, also der Parkplatz am Ende wieder um, nimmt wieder eins der Fahrzeuge raus, schaltet dann wieder zurück auf Rot. Das heißt, er arbeitet dieses Fahrzeug jetzt ab, sucht es hier am Parkplatz und so weiter. Und dann geht es auch im Netz wieder weiter. Es kann wieder ein Fahrzeug weitergeschickt werden. Und die Frage ist eben, wie wir das abbilden, kann man natürlich beliebig weiterspielen. Man kann sich auch überlegen, wie sieht das aus mit Prioritäten? Also was wäre jetzt, wenn… Wenn ein bayerischer Ministerpräsident kommt, dann werden plötzlich die Straßen teilweise gesperrt, damit der schnell durchkommt etc. Wie müsste man das im Internet machen? Kann man genauso hier in dem Beispiel durchführen. Schauen wir uns einen dieser Router an, also der Kreisverkehre, dann ist es immer so, dass wir praktisch bei jedem dieser Transitknoten, der hier in der Mitte dargestellt ist, sowohl einen Eingangspuffer als auch einen Ausgangspuffer haben. Und über diese beiden Puffer steuern wir praktisch, was drinnen passiert. Das sind nichts anderes als Warteschlangen, wo jetzt neue Pakete reinkommen. Wenn dieses Paket dann wirklich bearbeitet wird, wird es praktisch beim Ausgang über den jeweils richtigen Ausgang wieder rausgeschickt, dort auf eine andere Warteschlange gesteckt. Und wir können natürlich hier variieren mit der Größe der Warteschlange. Wir können mit der Geschwindigkeit der Abarbeitung und so weiter und so fort. Also es ist so, dass wir das… Diesen… Diesen Zustand bei jedem der Knoten haben und bei jedem der Router entsprechend überlegen müssen, wie wir den Verkehr durchleiten. Unsere Frage für die Transportschicht ist jetzt, was machen die Sender, um jetzt sicherzustellen, dass A, einerseits der Parkplatz am Ziel nicht überlastet ist, das ist die Flusskontrolle, und andererseits, dass wir auch das Transitnetz dazwischen nicht überlasten. Wenn alle jetzt wild drauf los sind… Dann hätten wir sofort einen Stau, der sehr schwer aufzulösen wäre. Das heißt, wir müssen versuchen, dass wir bei der Staukontrolle auch das Ganze mitmenschlich uns selbst unter Kontrolle halten, damit das Internet dementsprechend funktioniert. Schauen wir uns einmal an, welche Lösungsansätze es für die Flusskontrolle gibt. Also einen haben wir schon kennengeklärt, unsere Stop-and-Wait-Technik. Und die Stop-and-Wait-Technik war ja, dass wir immer ein Paket auf die Reise schicken. Und erst wenn das besteht… Und erst wenn das erledigt wurde, entsprechend das Nächste nachschicken. Das ist natürlich ein schöner Mechanismus, um auch jetzt diesen Verkehr in den Griff zu kriegen. Aber wir haben eben gesagt, die Auslastung des Netzes ist schlecht. Und das ist insbesondere dann schlecht, wenn man sehr lange oder sehr großes Roundtrip-Delay, das heißt, wenn wir sehr lange Rundlaufzeit haben. Das ist auch oszillierend, weil jetzt immer wieder so Spitzen drin sind, wo jetzt einer was rausschickt. Da könnte es natürlich sein, dass wir dann auch noch ein paar Rundlaufzeiten haben. Das ist auch oszillierend, weil jetzt immer wieder so Spitzen drin sind, wo jetzt einer was rausschickt. Da könnte es natürlich sein, dass alle fünf Häuser gleichzeitig schicken. Und dann haben wir das noch schlechter, als wir es vorher schon hatten. Die andere Methode, die wir auch schon kennengelernt haben, und da haben wir es kurz erwähnt, ist die Fenstertechnik. Das heißt, wir können uns von Anfang an überlegen, wie groß legen wir diese Puffer fest. Also das sind die festen Fenstergrößen. Oder wir passen vielleicht auch die Fenster dynamisch an. Das heißt, wenn wir sehen, dass mehr Verkehr im Netz ist, im Verkehrsnetz, in den Transitnetzen, dann machen wir die Fenster entsprechend kleiner. Oder umgekehrt, wenn weniger Verkehr ist, entsprechend größer. Das Hauptproblem liegt hier natürlich in der Fensterverkleinerung. Wie können wir dynamisch etwas kleiner machen, wenn es bereits fix in der Nutzung ist? Also auch das ist etwas, was man durchaus als Lösungsansatz sehen kann. Eine andere Sache, die wir bei der Flusskontrolle haben, und erinnern Sie sich, Flusskontrolle betrifft eben den Empfänger. Also das ist… Das ist praktisch der Parkplatz am Ziel. Und da könnte man natürlich auch sagen, okay, damit das nicht überlastet wird, machen wir doch eine Pufferreservierung vor der Sendung. Das heißt, der Sender schickt eine Nachricht, ich möchte dir etwas schicken. Und der Empfänger sagt dann, wie groß ist es? Und dann sagen wir dem Empfänger, wir brauchen einen Puffer in der Größe der Nachricht. Und erst wenn dieser Puffer verfügbar ist, dann darf die Nachricht geschickt werden. Was wir hier natürlich haben, sind zusätzliche Nachrichten und natürlich auch wieder die Verzögerung durch das Roundtrip Delay. Das heißt, es passiert halt wirklich erst etwas, nachdem die Pufferreservierung erfolgt ist. Umgekehrt können wir natürlich auch etwas machen, was wir vielleicht zu einer Art Kreditverfahren nehmen. Das heißt, wir können auch hier wieder bestimmte Puffer freigeben. Die kann man irgendwann im Laufe der Zeit reservieren. Und dann braucht man diese Credits über die Zeit hinweg auf, mit den Allokationsnachrichten. Und das Problem bei diesem Kreditverfahren ist, dass es halt etwas schwierig ist, wenn sehr viele Credits verteilt sind und dann etwas reinkommt, was man vorher vielleicht nicht gesehen hat. Also das Kreditverfahren hat immer das Problem, dass es nicht robust ist und dass man das sehr leicht in einen Zustand bringt, wo dann nichts mehr gesendet werden kann, weil eben die falschen Kommunikationspartner die ganzen Credits in der Hand halten. Und das letzte, was wir haben, ist zeitrasterabhängiges Senden. Das heißt, wir könnten natürlich auch sagen, wir senden nur dann etwas ins Netz, wenn wirklich mein Zeitslot erreicht ist. Jeder von den Sendern hat eine bestimmte Zeit, in der er mit dem Empfänger kommunizieren kann. Und nur zu diesen fixen Zeiten darf ich etwas rüber senden. Auch das ist natürlich schlecht, wenn ich sehr viele Teilnehmer habe und jetzt sehr lange warten muss, bis dass ich in meinem Zeitslot wieder drinnen bin. Schauen wir uns das in der Praxis an, wie das ausschaut mit eben unseren Sequenznummern und einem gewissen Puffer, den wir vorgesehen haben. Also dargestellt hier links im Bild der Sender, rechts im Bild der Empfänger. Und wir wollen jetzt Nachrichten vom Sender zum Empfänger senden. Das heißt, wir haben beim Empfänger irgendeinen Puffer mit einer bestimmten Größe. In dem Fall ist es 2x2 KB. Das heißt, 4K stehen da. Und am Beginn der Kommunikation ist dieser Puffer natürlich völlig leer und wartet jetzt auf Nachrichten. Das heißt, wir haben diese 2x2 KB, die jetzt zur Verfügung stehen. Und jetzt könnte der Sender irgendwann anfangen und sagen, okay, ich schicke mal 2K zum Empfänger rüber. Und das funktioniert ganz gut. Also er schickt die 2K auf den Weg mit der Sequenznummer, in diesem Fall 0. Der Empfänger empfängt das in seinem Puffer. Das heißt, die Hälfte des Puffers ist jetzt voll. Aber er hat noch Platz. Das heißt, er bestätigt, dass die Nachricht angekommen ist. Gibt gleichzeitig aber auch mit diesem ACK mit, wo er jetzt praktisch steht, in der Sequenznummer. Diese 2048 weisen darauf hin, dass wir jetzt die ersten 2K praktisch empfangen haben. Und das WIN ist ein Indikator dafür, wie viel noch zur Verfügung steht. Das heißt, wir teilen mit, was empfangen wurde. Das heißt, das erste 2K-Paket ist beim Empfänger angekommen. Belegt die Hälfte des Speichers. Und wir teilen auch noch mit, dass wir praktisch noch einmal 2K zur Verfügung haben. Der Sender ist dann so frech und sagt, super, nutze ich aus. Schreibe ich sofort wieder 2K. Schickt die wieder entsprechend auf den Weg mit der Sequenznummer 2048. Das ist ja die, die vom Empfänger entsprechend erwartet wird. Der empfängt es wieder in den Puffer. Und was passiert dann? Der Puffer ist voll. Das heißt, die Frage ist, wie reagiert jetzt der Empfänger? Der Puffer ist voll, was schreibt er zurück? Nun, er bestätigt mit dem Acknowledge, dass er praktisch jetzt auch die zweite Nachricht hat, wieder 2K empfangen. Deswegen ist die Sequenznummer bei der Bestätigung beim ACK jetzt 4096. Aber das Fenster, das Empfangsfenster wird auf 0 gesetzt. Und dieses Empfangsfenster auf 0 signalisiert dem Sender, dass er derzeit nichts senden kann, weil alles dort entsprechend voll ist. Dann vergeht einige Zeit, bis dass der Empfänger jetzt dazu kommt, den Puffer zu bearbeiten. Das wird jetzt irgendwann später der Fall sein. Er liest 2K in die Anwendung raus. Das heißt, die Anwendung ist bereit. Die holt sich jetzt das erste Paket, macht damit den Puffer wieder entsprechend frei. Und jetzt? Jetzt macht der Empfänger folgendes. Er bestätigt nochmal, ja, wir stehen immer noch bei diesen Sequenznummer 4096. Also das ist die nächste, die ich von dir erwarte. Aber unser Empfangsfenster hat jetzt wieder 2048 frei. Und das ist jetzt das Signal, wo er sagt, wenn das Win größer 0 ist, dann bedeutet es, der Empfangspuffer beim Empfänger hat Speicher verfügbar. Und wenn ich dann eine Nachricht wieder senden will, dann kann ich das im Anschluss eine derartige Bestätigung machen. Und was ich natürlich auch machen kann, ich kann einfach hergehen und sagen, ich schicke aber nur 1K rüber. Also nicht 2K in dem Fall, aber natürlich mit Sequenznummer 4096. Und dann wäre es so, dass wir praktisch nicht den ganzen verfügbaren Puffer vollschreiben, sondern in dem Fall nur die Hälfte des Puffers. Und da kann man natürlich dann schön hin- und herschieben. Und mit diesem einfachen Protokoll, bei dem wir eigentlich nur die Sequenznummern und die Quittungen nutzen, wie wir sie schon kennen, haben wir mit der Fenstertechnik auch die Möglichkeit, eben unser Puffermanagement zu machen und damit die Flusskontrolle. Wir brauchen also beim Sender und beim Empfänger Variablen. Und das eine ist eben, der Sender muss wissen, wie groß ist denn dieses Empfangsfenster. Also das ist das, was er auf die andere Seite rüber schicken kann. Das Empfangsfenster ist dementsprechend kleiner als der gesamte Receive Buffer, also wie viel Puffer überhaupt beim Empfänger zur Verfügung steht. Und wir sehen da in diesem Bild schematisch dargestellt, dass der blaue Bereich die Menge der Daten ist, die noch nicht bearbeitet worden sind. Das heißt im Endeffekt haben wir hier einen Speicher, wo man von zwei Seiten reinschreiben kann. Der Dateneingang ist das, was vom Netz reinkommt. Und die Datenabnahme erfolgt durch den Anwendungsprozess. Auf der anderen Seite von dem, was im Speicher schon drin ist.

belegt ist. Kommen wir zurück zum Sender. Der Sender weiß also, wie groß ist das Receive-Window. Das wird ihm ja mitgeteilt mit der letzten Bestätigung, haben wir gesehen. Er weiß, was das letzte Byte ist, das er gesendet hat. Er weiß aber auch, was das letzte Byte ist, das bestätigt worden ist. Und von dem kann er ableiten, was er als nächstes senden kann und wie viel überhaupt beim Empfänger ankommen kann. Das ist die Flusskontrolle. Umgekehrt hat natürlich der Empfänger den gesamten Receive-Buffer zur Verfügung. Und der unterscheidet jetzt zwischen zwei Dingen. Und zwar das eine ist, wie viele Bytes hat er denn tatsächlich empfangen, aber noch nicht gelesen. Und dann muss er auch wissen, wie viel hat dann der Anwendungsprozess jetzt schon rausgeholt aus dem, was empfangen worden ist. Und das ergibt dann praktisch das Empfangs-Window, das Receive-Window, wie wir es hier sehen. Das ist nämlich die Größe des Empfangs-Buffers minus dem, was gerade gelesen wurde abzüglich des letzten Bytes, das von der Anwendung bereits rausgeholt worden ist. Das heißt, das Receive-Window ist davon abhängig, wie schnell der Anwendungsprozess das rausholt. Und der Sender weiß davon immer, wie groß denn der Bereich ist, wie groß denn die Nachricht sein kann, die er entsprechend durchsenden will. Diese Beziehung zwischen dem letzten Byte, das gesendet worden ist und dem letzten Byte, das bestätigt worden ist beim Sender, haben wir schon dargestellt. Das ist eben das, was wir sagen, wie viel wollen wir überhaupt drüber schicken. Und was wir natürlich hier auch machen können, ist, wir können auch bei den Quittungen wieder mehrere Segmente auf einmal nehmen. Also solange der Sender im Griff hat, dass er den Receive-Buffer nicht völlig ausnützt oder überschreibt, mehr rüber schickt, dass dort empfangen werden kann, kann man dem Sender auch zutrauen, dass er praktisch mehrere Segmente schickt und dann eben, Summenquittungen erhält, in denen diese mehreren Segmente entsprechend empfangen worden sind. Und das ist jetzt schon die erste Maßnahme, die wir kennenlernen wollen, die Flusskontrolle. Also eine einfache Möglichkeit, um die Überlastung des Empfängers zu verhindern, zu vermeiden und der Sender hat das in der Hand, indem er einfach diesen Instruktionen vom Empfänger entfolgt. Also Teil 1 in dem Sinne abgeschlossen. Teil 2 ist ähnlich. Die Congestion Control bzw. die Staukontrolle. Nur haben wir jetzt das Problem, es geht nicht um die Überlastung des Empfängers. Das heißt, es ist nicht der, den wir jetzt auf unserer Transportschicht gegenüber haben, also das Peer Entity, das auf der anderen Seite entsprechend drüber steht, sondern es ist etwas, was eigentlich nur sich auf den unteren Schichten manifestiert. Und das ist die ganze Schwierigkeit, die wir jetzt lösen müssen. Es gibt ja auch viele andere Unternehmen, die irgendwie die Einnahmen systematisch einsetzen müssen, also wir haben ja schon die Vorbereitung, dass wir uns da einigen Leuten sprechen müssen. Das heißt, wir sind wieder bei unserem ursprünglichen Beispiel, wo Alice und Bob etwas senden will, aber wir haben leider noch andere, wie zum Beispiel die CARROL, die das Internet auch nutzen und gleichzeitig Pakete ins Netz rein spielen. Wir haben ja schon gesagt, Alice und Bob sind ja auf der Transportschicht direkt durch die Ende zu Ende Verbindung miteinander in Kontakt. Die sehen eigentlich nicht, was die CARROL macht oder wo die CARROL herkommt. kommt. Die bekommen das praktisch nur mit, dass jetzt vielleicht jemand anderer zu viel einspeist und wir müssen jetzt überhaupt einmal im ersten Schritt überlegen, wie kriegen wir denn, wie stellen wir denn diese Überlastung der Netze überhaupt her? Also wir haben schon gesagt, zu viele Transportinstanzen auf vielen unabhängigen Endsystemen, die wir nicht beeinflussen können, schicken gleichzeitig viele Pakete ins Netz und das Netz oder die Router im Netz, im Transitnetz reagieren genauso bei der Überlastung. Das heißt, was wir eigentlich haben, ist eine Überlastung der Router auf der Schicht darunter, auf der Vermittlungsschicht, die wir uns ja nächstes Mal anschauen werden, aber wir merken das jetzt trotzdem auf der Transportschicht und das ist schon die erste Frage, wie können wir denn überhaupt feststellen, ob eine Überlastung der Transitnetze besteht, wenn wir auf unserer Schicht eigentlich gar keine Transitnetze sehen? Also da sind wir jetzt genau bei diesem Problem oder beziehungsweise dem Feature, wo wir gesagt haben, die Schichten sind transparent, also das, was darunter steht, steckt sehe ich auf der aktuellen Schicht einfach nicht, aber jetzt ist es so, wir wollen das eigentlich wissen, wenn darunter was passiert und wenn wir erst einmal festgestellt haben, dass es eine Überlastung der Transitnetze gibt, dann müssen wir uns natürlich überlegen, wie wir darauf reagieren können. Betrachten wir mal die erste Frage, wie können wir das feststellen? Und da ist natürlich ganz klar, wir könnten uns ein eigenes Protokoll überlegen, das jetzt von der Vermittlungsschicht drauf gibt, dass es überlastete Router gibt. Also, dass es praktisch zu einer Überlastung gekommen ist und wir auf der Transportschicht jetzt einfach ein bisschen uns zurückhalten sollten beim Netzwerkverkehr, den wir erzeugen. Und so ein Protokoll gibt es, das ist das XCP, das Explicit Congestion Protokoll, da steckt ja auch der Stau, die Congestion schon drinnen und das teilt uns genau das mit. Das ist also ein Protokoll, das darunter läuft, wenn ich dieses Protokoll in meine Bearbeitung einbaue und das auslese, dann kriege ich praktisch Hinweise, dass das darunter überlastet ist. Ich kann mir aber auch anders helfen, wenn ich dieses Protokoll nicht habe, indem ich auf indirekte Indikatoren zurückgreife. Und wir kennen indirekte Indikatoren dahergehend, dass wir gesagt haben, auf unserer Schicht wollen wir zuverlässigen Transport durchführen. Wir haben aber auch die Möglichkeit, dass auf unteren Schichten Segmente weggeworfen werden, dass wir den Segmentverlust haben. Und den Segmentverlust merken wir auf unserer Schicht, wenn die abgeschickten Segmente auf der anderen Seite nicht ankommen und wir irgendwo in einen Timeout laufen oder keine Bestätigung für die Ankunft der Nachricht bekommen. Das ist dieser Segmentverlust. Also das ist etwas, was auf den unteren Schichten passiert, wo wir auch aber auf unserer Transportschicht Indikatoren haben, die uns mitteilen, dass das jetzt vorgekommen ist. Und dann können wir darauf reagieren, auch wieder mit einer Reihe von Lösungsansätzen. Einer wäre zum Beispiel, dass man sagt, okay, damit man überhaupt gar nicht in das Problem des Staukontrollen kommt, machen wir virtuelle Kanäle. Virtuelle Kanäle, das ist vergleichbar mit unserem Verkehrsbeispiel, wie wir es gerade gehabt haben, dass jeder seine eigene Spur kriegt. Wenn man jetzt von Garching nach München reinfahrt und man hat eine eigene Spur, dann hat man keinen Stau deswegen. Es ist aber auch sofort klar, dass das sehr große Ressourcenvergeudung ist und dass die Aufbauzeit auch relativ lang dauert. Wenn ich so einen virtuellen Kanal entsprechend aufbauen muss, dann muss ich ja den ganzen Weg durchgehen und jedem am Weg mitteilen, jetzt kommt gleich etwas, reserviere das entsprechend und stelle sicher, dass die Nachricht entsprechend durchgeht. Wir können uns überlegen, dass wir sagen, wir begrenzen die Anzahl der Verbindungen pro Host, pro Prozess. Wenn man sagt, wir haben drei Teilnehmer am Netz, dann kriegt jeder nur mehr die Möglichkeit, ein Drittel des Netzes auszunutzen. Auch das ist natürlich in gewissem Maße eine Einschränkung des Verkehrs, den wir erzeugen und dementsprechend auch etwas, was vielleicht nicht ganz so günstig ist. In der Praxis macht man das, wenn wir sehen, dass irgendwelche Netzteilnehmer sehr viel Verkehr erzeugen. Also wenn jetzt zum Beispiel der Hochenergiephysiker vom CERN nach München eine große Menge an Daten überträgt, dann würden wir wahrscheinlich irgendwie sagen, okay, dem geben wir aber nur 80 Prozent des Netzes und die anderen 20 können dann immer noch von den übrigen Teilnehmern benutzt werden. Also das ist so eine Möglichkeit, die wir haben. Wir können aber natürlich auch begrenzen, wie viel das man jetzt reinschickt ins Netz, also die Senderate. Da gibt es mehrere verschiedene Lösungen. Traffic Contract, klar, da ist es dann vertraglich festgelegt, wie viel das ich reinschicken kann. Oder das sogenannte Leaky Bucket Verfahren. Also Leaky Bucket kann man sich einfach so vorstellen, wir haben einen Eimer und in dem Eimer ist ein Loch unten drinnen und es darf immer nur das ins Netz reinfließen, was jetzt durchs Loch durchtropft. Und wenn das Loch klein genug ist, dann kann das auch nicht passieren, dass das Netz darunter entsprechend überlastet wird. Auch das sind Verfahren, wo man jetzt in der Kürze der Zeit oder in der Vorlesung nicht eingehen wollen, aber das sind lauter Verfahren, die halt irgendwann einmal jemand ausprobiert oder auch sogar vorgeschlagen hat. Wir können auch auf der anderen Seite sicherstellen, dass wir nicht nur begrenzen, wie viel reingeht, sondern auch sicherstellen, dass alles rausgeht aus dem Netz. Das heißt, wir haben garantierte Abnahmenraten beim Empfänger. Der Empfänger empfängt einfach bis zu einer gewissen Menge die Daten, die kommen und damit nimmt er das raus aus dem Kreislauf, aus dem Transitnetz, was dort auftritt. Ist natürlich auch eine gewisse Schwierigkeit, weil man dann den Empfänger entsprechend groß darstellen muss. Oder, dass man überhaupt auch sagt, vielleicht können wir das Ganze zusammenfügen, größere gemeinsame Pakete machen und diese gemeinsamen Pakete entsprechend auch abdecken, die dann dort auseinanderwerfen. Nächstes Verfahren ist konstante Last im Netz. Also hier ist es so, dass wir sagen, okay, es wird einfach nie mehr Last erzeugt, als es überhaupt möglich ist. Da gibt es so etwas wie ein Taxi-Verfahren. Kann man sich auch wieder vom Namen relativ einfach vorstellen. Anstatt, dass jetzt jeder wie wild etwas ins Netz reinschickt, begrenzen wir die Anzahl der Fahrzeuge. Also wenn wir jetzt sozusagen das Singapur-Modell nehmen. In Singapur hat man gesagt, man hat nur für die ganze Stadt eine gewisse Anzahl von Lizenzen für Fahrzeuge zur Verfügung. Wenn man nicht mehr Fahrzeuge in der Stadt erlaubt, dann hält man den Stau auch in gewissen Grenzen zurück. Und das könnte man mit so einem Taxi-Verfahren, wo man sagt, wir schicken jetzt nur noch Taxis auf den Weg und wenn es eben kein Taxi gibt oder wenn das Netz überlastet ist, dann sind ja alle Taxis schon unterwegs, dann kriege ich auch wieder nichts raus. Und der letzte Punkt ist natürlich auch, dass man eigene Füllungsregeln aufstellt pro Verbindung. Das heißt, wir überlegen uns, was entsprechend passiert und schauen, dass das jetzt pro Verbindung nicht überschritten wird. Also das ist ähnlich, wie wir es auch oben gehabt haben mit den virtuellen Kanälen, aber dass man die Regeln vielleicht auch noch dynamischer anpasst an das, was gerade los ist. Im Internet haben wir schon gesagt, ist es so, dass wir wirklich einfach bei Überlast reagieren. Und zwar, wir schauen nochmal, wenn Überlast passiert, dann werden Segmente verworfen. Und dann müssen wir praktisch darauf reagieren. Und das werden wir uns jetzt noch im Detail ein bisschen näher anschauen, was passiert denn jetzt, wenn jetzt so ein Segment verloren wird. Wir können im Endeffekt als Reaktion auf die Indikatoren unser Verhalten ändern. Also angenommen, wir haben so ein verlorenes Segment, das lässt sich eben dadurch erkennen, indem wir ein Timeout haben oder indem Nachrichten bestätigt werden, die vorher schon bestätigt worden sind. Also die Ergduplikate, die wir haben. Da ist der Stau implizit praktisch erkannt. Das heißt, irgendwas passiert, irgendein Stau ist im Netz. Und wenn das der Fall ist, dann können wir auf Senderseite reagieren, indem wir einfach sagen, wir verringern die Senderate. Ganz einfache Reaktion darauf und das ist jetzt das Verhalten, das wir durchführen können. Umgekehrt ist es natürlich so, wenn wir das wirklich bestätigt bekommen mit einem ARC, dann wissen wir, oh, das ist gut durchgegangen. Kein Timeout ist aufgetreten. Das heißt, die Nachricht ist beim Empfänger angekommen. Dann können wir eigentlich so weitermachen, wie wir wollen. Beziehungsweise wir könnten sogar sagen, naja, solange es nicht schief geht, schickt man einfach jetzt ein bisschen mehr auf den Weg und schauen, ob das immer noch funktioniert. Und tatsächlich ist das das, was das Internet praktisch probiert. Das Internet macht immer diesen Versuch, wie weit kann ich denn gehen mit der Senderate? Wie groß kann ich die raufstellen? Und wie können wir durch kontinuierliches Probieren versuchen, das Netz so gut wie möglich auszunützen? Also Senderate entsprechend erhöhen, bis ein Segment verloren geht. Wann das Segment verloren geht, dann reagieren wir darauf, indem wir die Senderate wieder verringern. Wenn es dann wieder gut durchgeht, erhöhen wir wieder entsprechend und warten, bis das Segment wieder verloren geht und so weiter und so fort. Also das ist dieses Ping-Pong zwischen diesen beiden Seiten. Und dieses reaktive Verhalten ist tatsächlich, dass das umgesetzt wurde bei der Überlastkontrolle beim TCP. Und zwar passiert das eben in drei Schritten. Also wir beeinflussen oder wir stellen diese Überlastkontrolle die Staukontrolle damit sicher, indem wir sagen, wir kontrollieren das über die Senderate ins Netz. Wir fangen zuerst einmal langsam an. Also das nennen sie Slow Start, langsamer Start. Wir beginnen langsam Nachrichten ins Netz reinzuschicken und steigern uns dann exponentiell. Das heißt, wir schicken zuerst einmal ein Paket rein, dann schicken wir zwei Pakete rein, dann vier Pakete und so weiter und so fort. Und das machen wir so lange, bis dass wir bei einem gewissen Wert angekommen sind, wo wir glauben, dass das Netz dann voll ist. Wenn das Netz voll ist, werden wir ein bisschen vorsichtiger. Dann gehen wir zu Stufe 2 und machen mit linearem Wachstum weiter. Das heißt, wir versuchen zwar immer noch mehr zu schicken, aber das ganz vorsichtig, indem wir eben die Überlast vermeiden. Das heißt, wir schicken immer nur ein bisschen mehr und schauen, geht es noch. Wenn es nicht geht, fallen wir wieder entsprechend zurück. Und der nächste Punkt ist eben genau der, der dann passiert, wenn wir dann in die Überlast reinlaufen, dann müssen wir uns überlegen, wie wir davon wieder loskommen. Es gibt auch verschiedene Verfahren. Eines ist dieses Fast Recovery, die schnelle Erholung. Das heißt, wir merken, oh, jetzt ist Stau passiert. Wir fallen zurück in dem, was wir praktisch als Senderate im Netz haben und müssen dann von dort wieder fortsetzen entsprechend. Schauen wir uns die Verfahren an und schauen wir uns zuerst einmal wirklich nur die ersten zwei an. Was passiert mit den beiden und können wir mit den beiden auch schon etwas lösen. Das heißt, wir brauchen zur Vorbereitung wieder einmal eine Reihe von Variablen und die Variablen brauchen wir eigentlich bei den ganzen TCP Instanzen. Das heißt, die eine Variable ist ein Congestion Window, also wieder Fenstertechnik, wie wir es schon kennen. Und dieses Congestion Window setzt sich eben daraus zusammen, dass wir sagen, wir schicken nicht mehr, als wir jetzt in unserem Fenster entsprechend drinnen haben. Die Einspeisung hängt davon ab, wie groß denn jetzt das aktuelle Fenster ist. Das effektive Fenster ist natürlich das, was bestimmt wird durch das, was auf der Empfängerseite überhaupt empfangen werden kann, also Flusskontrolle und dem, was über unser Congestion Window reinkommt. Das heißt, wir müssen da auch ein bisschen unterscheiden zwischen den beiden Verfahren. Wir wollen den Empfänger nicht überlasten, wir wollen auch nicht das Netz überlasten. Und das, was wir reinschicken, hängt eben dann davon ab, wie groß die beiden Fenster entsprechend sind. Wir können dann auch sagen, wie groß die maximale Datenmenge pro Segment ist, also auch da können wir natürlich variieren, wieviel wir auf die Reise schicken und wir legen uns einen Schwellwert fest, also einen Threshold, eine Schranke für das schnelle Wachstum des Überlastfensters. Das heißt, es ist dieser Wert, wo wir sagen, gut, die Erfahrung hat gezeigt, bis dahin funktioniert das Netz immer gut und auch dann müssen wir entsprechend vorsichtiger sein. Und

Schlussendlich brauchen wir eben auch noch den Timer, den Retransmission-Timer. Nach Ablauf des Timers werden nicht bestätigte Segmente neu gesendet. Das ist jetzt einfach ein Timer, der feststellt, habe ich schon für alle Pakete meinen Empfang bestätigt bekommen. Und wenn dieser Timer praktisch ausläuft, dann beginne ich nicht neue Pakete zu senden, sondern ich sende die Segmente noch einmal, die ich eben bisher noch nicht bestätigt habe. Und auch da sehen wir wieder eine Reihe von Optimierungen. Das heißt, was wir uns jetzt anschauen, ist ein erstes Verfahren, wo wir sagen, wie kann so etwas funktionieren mit dieser Staukontrolle, wie wir es jetzt haben, und der Idee, dass wir das nicht so gut wie möglich ausnutzen, bis das etwas passiert. Das heißt, der erste Schritt ist, wir schauen uns die Senderate bei einem Netz ohne Last an. Und dieses Verfahren, das wir jetzt kennenlernen, nennt sich TCP-Tacho. Das heißt, das ist einfach der Name von diesem Netz. Und der erste Schritt, wie schon gesagt, ist dieser Slow Start. Das heißt, wir beginnen mit einem möglichst kleinen Überlastfenster. Also das kleine Überlastfenster bedeutet, wir haben jetzt Nachrichten der Länge 1, die wir rausschicken. Und das setzen wir mal als ersten Versuch. Also der erste Schritt ist immer ganz klein. Wir fangen mit ganz kleinen Nachrichten an, schicken die entsprechend raus. Wenn wir jetzt das erledigt haben, das erste Paket entsprechend auf dem Weg ist, dann machen wir weiter. Dann sagen wir, jetzt verdoppeln wir das, was wir drinnen haben. Das heißt, für jede Bestätigung, die wir von der anderen Seite kriegen, wissen wir, das Netz ist in einem guten Zustand, das Transitnetz hat funktioniert. Das kommt die Bestätigung vor dem Retransmission-Timer. Das heißt, Paket angekommen, Netz ist in gutem Zustand, los geht's. Dann schicken wir gleich einmal doppelt so viel und schauen, ob es das auch noch aushält. Das heißt, wir gehen jetzt von diesem Slow Start in das exponentielle Wachstum über und verdoppeln jeweils die Menge, die wir rausschicken. Und das sehen wir hier auch ganz schön in dem Bild dargestellt. Das heißt, wir fangen ganz unten an mit dem 1 MS. Das ist auf der vertikellen Achse. Das ist das Congestion Window. Wir sehen eingezeichnet den Schwellwert, den Threshold. Das heißt, bis dahin haben wir gute Erfahrungswerte gemacht. Und jetzt schauen wir, dass wir so schnell wie möglich mit der Menge der Daten wieder dorthin kommen. Das heißt, bis dahin hat das Netz gut funktioniert. Also schauen wir, dass wir auch wirklich so viel reinschicken. Wir beginnen sehr klein. Steigern uns aber exponentiell und sind dann irgendwann bei diesem Schwellwert angekommen, der sich aus der Erfahrung oder eben aus den Kapazitäten des Netzes ergeben hat. Wir können uns das dann auch entsprechend hier anschauen. Das funktioniert so mit dem Slow Start. Host A, Host B, also Sender, Empfänger. Host A schickt ein Segment. Das wird bestätigt. Also wenn 1 bestätigt wird, weiß er, Netz ist in gutem Zustand, dann schickt man sofort 2 noch. Das sehen wir dann auch im 2. Schritt. Auch die 2 werden wieder bestätigt, weil das Netz immer noch in gutem Zustand ist. Wenn 2 funktioniert hat, dann sind wir riskant, gehen sofort auf 4 Segmente und so weiter und so fort. Also wir erhöhen immer die Anzahl der Segmente auf den ganz einfachen Punkt, dass wir von den Bestätigungen ableiten können, dass das Netz in gutem Zustand entsprechend ist. Und wenn wir jetzt dann zurückkommen, dann sind wir eben irgendwann durch das exponentielle Wachstum relativ schnell bei diesem Schwellwert. Das ist die Hoffnung. Das heißt, wir sind schnell bei dem Wachstum. Wir sind schnell bei dem Wert, wo unser Netz ausgelastet ist. Und jetzt müssen wir aufpassen und vorsichtig sein. Wir wissen, dass wir jetzt in einen Bereich kommen, wo eine Überlastung stattfinden können. Solange es nicht stattfindet, die Überlastung, steigern wir uns nach wie vor. Das heißt, wir erhöhen das Paket linear immer noch weiter. Also nicht mehr exponentiell, sondern jetzt mehr mit einfachen Versuchen und steigern die Rate dessen, was wir ins Netz rausgeben. Das ist jetzt das lineare Wachstum. Der zweite Schritt. Mit dem Bewusstsein, dass irgendwann jetzt die Last im Netz zu hoch sein wird und wir im Prinzip gewisse Segmente verlieren werden. Und verlieren hören wir deswegen, weil eben der Timeout erreicht wird, bevor die Bestätigung für ein gesendetes Paket zurückkommt. Und das müssen wir entsprechend in den Griff kriegen. Das heißt, wir sind bis jetzt in der sicheren Zone des Netzes ohne Last. Das heißt, bis jetzt haben wir noch keine Schwierigkeiten gehabt. Und jetzt schauen wir uns das an, was passiert, wenn das Netz plötzlich eine Last hat. Last bedeutet, wir haben einen Timeout. Das heißt, der Timeout ist erreicht. Wir haben innerhalb des vorgegebenen Zeitintervalls, das ist eben der Timer, keine Bestätigung erhalten. Dann müssen wir vorsichtig sein. Dann reduzieren wir die Senderate. Und zwar, was wir als erstes machen, wir machen zwei Dinge. Erstens sind wir um eine Spur vorsichtiger. Vielleicht war unser Schwellwert zu hoch. Also vielleicht müssen wir einfach, weil das Netz jetzt so stark beansprucht ist, den Schwellwert verringern. Und den verringern wir auf die Hälfte des Congestion Windows. Und dann ist eben die Frage, wie wir neu starten. Und bei TCP Tahoe ist es so, dass wir wieder mit Slow Start. Das heißt, wir gehen wieder ganz am Anfang zurück. Und das sehen wir jetzt hier sehr schön dargestellt. Das heißt, wir haben hier das ganze Verfahren TCP Tahoe. Wir beginnen ganz am Anfang, wie gesagt, sehr langsam mit dem Slow Start. Ganz kleine Pakete steigern uns exponentiell bis zum Schwellwert. Dann gehen wir vorsichtiger vor, steigern uns linear weiter. Und dann kommt zu irgendeinem Zeitpunkt eben der Timeout, der Retransmission Timer. Das heißt, wir haben gesagt, das Paket wurde nicht bestätigt innerhalb der vorgegebenen Zeit. Was dann passiert ist, wir wissen, ups, wir haben jetzt eine Netzüberlastung. Jetzt müssen wir darauf reagieren. Und damit das Netz weiter funktionieren kann. Fangen wir wieder ganz vorsichtig an. Das heißt, wir gehen jetzt wieder runter von dieser Phase zwei vom linearen Wachstum auf die Phase eins zurück zum Slow Start. Beginnen dort vom neuen, setzen aber sicher auch den Schwellwert runter auf die Hälfte des am Schluss beobachteten oder dem Schluss noch erfolgreichen Congestion Windows. Das heißt, wir sind jetzt auch wieder zwei zuerst vom Slow Start, dann exponentiellen Wachstum. Aber wir wachsen. jetzt wesentlich vorsichtiger und nur mehr eben bis zur hälfte von dem was funktioniert hat ob da geht es dann wieder entsprechend linear weiter also das ist der tcp tahoe algorithmus und wir sehen sehr schön dass man praktisch wirklich versucht ist nicht so schnell wie möglich auszulasten wenn wir die auslastung merken dann wäre wieder konservativ gehen zurück und fangen vorsichtig wieder an mit dem slow start entsprechend also was man da jetzt nicht drinnen gehabt haben ist dieses fast recovery das sehen wir gleich wie können wir denn jetzt optimieren wenn so ein verlust auftritt wir haben gesagt wenn wir diesen timeout erreichen dann hüpfen wir zurück auf das low start wir fangen wieder mit klein an das heißt grundlage ist die bei segment verlust werden alle darauf empfangenen segmente mit der gleichen quittungsnummer quittiert hat das ist die letzte die funktioniert hat und egal was man jetzt dann empfangen haben es kommt nicht mehr durch wir schicken einfach jetzt einmal die gleiche quittungsnummer und diese quittungsduplikate weisen uns frühzeitig darauf hin dass der segmentverlust passiert ist vielleicht haben wir den den timer noch gar nicht erreicht wir kriegen aber jetzt plötzlich quittungsduplikate von nachrichten die wir rüber geschickt haben das heißt es werden nicht dass das letztes gesendete paket quittiert sondern es werden pakete davor mehrfach quittiert für jede nachricht die entsprechend rübergegangen ist das heißt die quittungsduplikate treten in der regel noch auf bevor der timeout passiert das ist schon eines was man in unsere optimierung einfließen lassen können wenn wir nämlich nicht nur auf den timeout warten sondern wenn wir plötzlich quittungsduplikate entsprechend beobachten das heißt was wir hier machen können ist in dem moment wo wir quittungsduplikate sehen fangen wir sofort dann pakete einfach noch mehr auszuschicken das fast three trans mit also der ansatz sagt nichts anderes dass wir haben jetzt durch irgendeinen Fehler in den Transitnetzen praktisch Segmentverlust gehabt. Naja, wenn die sowieso verloren sind, dann schicken wir es doch sicherheitshalber noch einmal auf den Weg. Das ist das Fast Retransmit. Die andere Geschichte ist auch, dass wir sagen können, wir können nicht nur Fast Retransmit, sondern wir machen Fast Recovery. Das heißt, wir stellen sehr schnell wieder einen Zustand her, mit dem das Netz weiter funktioniert. Schauen wir da ein bisschen weiter rein. Fast Retransmit, der Sender wiederholt beim Empfang eines Quittungsduplikats sofort alle nachfolgenden Segmente. Das heißt, er sieht, es kommt jetzt eine Quittung noch einmal rein für ein gesendetes Segment, das eigentlich vorher schon bestätigt worden ist. Und wenn das passiert, schickt man einfach alles, was danach noch einmal gekommen ist, noch einmal raus. Und das bevor wir den Timeout überhaupt erreichen. Wir sehen das hier auch sehr schön. Das ist wieder ein Bild aus Kupfer. Sie haben rechts oben den entsprechenden Literaturhinweis. Und wir sehen hier sehr schön, da werden jetzt fünf Nachrichten rausgeschickt. Zuerst eine mit 8 Byte, dann eine mit 20 Byte, dann 15 Byte, 6 Byte, 16 Byte. Und wir sehen, die erste geht einmal problemlos durch. Es wird bestätigt, dass diese 8 Byte angekommen sind und die nächste zu erwartende Sequenznummer ist die 100. Die wird aber gleichzeitig schon auf den Weg geschickt. Und was wir sehen ist, die Sequenznummer 100 geht irgendwo verloren. Das ist das X, was wir da mitten in der Zeichnung entsprechend sehen. Danach kommt das Segmentnummer 120. Der Host A, wie gesagt, schickt einfach einmal gleich raus, ohne zu warten. Und das Segment mit der 120 wird aber noch einmal bestätigt. Das kommt also an, wird bestätigt mit der 100. Das heißt, der Host A wird stutzig. Jetzt habe ich ihm gerade praktisch 100 und 120 geschickt und er bestätigt mir das 120. Mit der 100. Die 100 habe ich aber vorher schon gehabt, nämlich mit der Sequenznummer 92. Das heißt, zu dem Zeitpunkt weiß er schon, ob jetzt habe ich ein Quittungsduplikat. Dann ist irgendwas passiert. Und tatsächlich sehen wir, dass der Timeout noch gar nicht erreicht ist. Der läuft noch. Bevor dem Ende des Timeouts auf der linken Seite beim Host A, schicke ich einfach noch einmal das Paket, das verloren gegangen ist. Sequenznummer 100. Schicke das noch einmal raus. Das fast retransmit. Also wir schicken erneute die Segmente, die verloren gegangen sind, bevor überhaupt der Timer abgelaufen ist. Und dadurch, dass wir nicht auf das Ablaufen des Timers warten, sind wir natürlich entsprechend schneller. Problem ist natürlich, dass das nach wie vor sehr viel Traffic erzeugt, wenn wir einfach im Netz drinnen haben. Aber wir sehen ja hier im Beispiel auch, dass das durchaus funktionieren kann. Die zweite Möglichkeit ist eben dieses Fast Recovery. Das heißt, das Sendeverhalten nach vorne. Nach schneller Neuübertragung. Pro empfangenen Quittungsduplikat wird wieder ein Segment neuer Daten geschickt. Das heißt, wir sehen ein Quittungsduplikat und dann schicken wir noch einmal eins, das nicht funktioniert hat. Aber das Congestion Window wird nach wie vor respektiert. Das heißt, wir verändern das. Wir machen es nicht kleiner, sondern wir sagen, okay, da ist jetzt irgendwas passiert. Aber wir schicken jetzt einfach noch einmal das Segment raus und dann wird es schon wieder passen. Und beim dritten Segment, das jetzt doppelt bestätigt wird. Also beim dritten Quittungsduplikat müssen wir jetzt das Congestion Window neu berechnen. Außerdem lassen wir dieses Slow Start weg. Das heißt, wir fallen nicht immer ganz runter auf den untersten Punkt, sondern wir fallen einfach wieder zurück auf den Schwellwert. Und verringern aber dann notfalls eben den Schwellwert, wenn wir sehen, dass das zu keinem Erfolg führt. Das sehen wir auch hier in dem Bild sehr schön dargestellt. Also wir haben mit Blau das TCP Tahoe. TCP Tahoe, also wie gesagt, wir fangen an. Slow Start, exponentielle Steigerung, lineare Steigerung. Wir sehen dann, bei zwölf Segmenten haben wir ein Problem mit der Nachrichtenübertragung. Das heißt, wir fallen bei TCP Tahoe wieder ganz zurück auf ein Segment. Bei TCP Reno sagen wir, nein, wir fallen nicht ganz zurück, sondern wir gehen einfach wieder zu dem zurück, was wir schon als Schwellwert kennengelernt haben. Nämlich bis zu unserem Threshold und machen von dort entsprechend wieder zurück. Und wir sehen dann natürlich sofort in diesem Bild, dass TCP Reno als Verbesserung von dem Tahoe-Algorithmus wesentlich besser in der Auslastung des Netzes ist. Weil es eben nicht ganz aufs Ende zurückfällt. Also auch hier wieder einfach noch einmal durchdenken. Sie werden dann selbst drauf kommen, dass das eine Möglichkeit ist, wie man das machen kann. Und TCP Tahoe und TCP Reno sind halt Beispiele, wo man ein bisschen versucht, das besser zu verstehen. Wir können auch hier in diesem Zustandsdiagramm wieder von Kurose Ross praktisch sehen, wie das Ganze entsprechend funktioniert. Wir haben die drei Zustände Slow Start, Congestion Avoidance und Fast Recovery. Also das sind die drei Bereiche, die wir ja vorher auch als Methoden kennengelernt haben. Wir haben gesagt, bei TCP Tahoe war es eben nur Slow Start und Congestion Avoidance. Also wir beginnen eben vorsichtig. Steigern uns etwas. Das ist exponentiell. Und wenn wir dann im linearen Bereich sind, versuchen wir die Congestion zu vermeiden. Problem könnte natürlich sein, dass wir wieder zurückfallen auf den Slow Start. Die Alternative ist, dass wir nicht vom Congestion Avoidance wieder auf den Slow Start gehen, sondern runter auf das Fast Recovery. Das heißt, wir steigen gleich etwas weiter oben ein. Und das ist eigentlich ganz ein gutes Schaubild. Wenn man das einmal durchdenkt. Wenn man das verstanden hat. Dann versteht man auch die Optimierungen, die wir hier haben. Und diese Optimierungen sind ja in der Praxis auch umgesetzt. Was man tatsächlich macht, ist, dass man noch einen Schritt weiter geht. Nämlich mit der TCP Stauvermeidung nach dem AIMD Prinzip. Additive Increase, Multiple Decrease. Das heißt, wir vergessen das mit dem Slow Start. Sondern wir machen grundsätzlich immer diesen linearen Anstieg. Denn wir haben das Additive Increase. Und bei Paketverlust. Machen wir eine Halbierung unseres Congestion Windows um den Faktor 2. Das ist das Multiple Decrease, das wir jetzt haben. Und dadurch bewegen wir uns immer praktisch an der Grenze dessen, wo es das Netz hergibt. In Zyklen auf und ab. Das sehen wir sehr schön auf dem Bild auch dargestellt. Also die Zeitachse horizontal. Die Größe des Congestion Windows vertikal. Und wir sehen, wir bewegen uns praktisch aufwärts. Linear. So lange wie möglich. Und wenn dann etwas passiert. Verringern wir das Congestion Window auf die Hälfte. Das wir hier beobachtet haben. Dann steigern wir uns wieder linear. Bis das Netz wieder überlastet ist. Wenn die Überlastkontrolle kommt. Also Duplikate bei den Quittungen. Oder entsprechend der Timeout. Dann fallen wir auf die Hälfte des Congestion Windows wieder zurück. Beginnen dort wieder. Und das ist eigentlich sozusagen der Ansatz. Mit dem auf und ab. Den wir hier entsprechend durchführen. Und mit dem wir uns immer innerhalb dem bewegen. Was das Netz entsprechend hergibt. Bis das wir eben den Stau durch die indirekte Beobachtung von Paketverlust. Oder eben konkret von Quittungsduplikaten. Oder von erreichendes Timeout sehen. Das kann man natürlich jetzt nicht beobachten.

auch umlegen auf die aktuelle Situation mit der Corona-Pandemie, also hier ist es auch wieder so, praktisch wir lassen das laufen, bis ein gewisser Wert erreicht ist, der Wert, der erreicht ist, ist also dieser Reproduktionsfaktor 1, wenn wir sehen, dass wir jetzt in die Gefahr laufen, dass unsere Krankenbetten überlastet sind, dann fallen wir wieder zurück, indem wir wieder irgendwelche Restriktionen einsetzen, machen dann entsprechend wieder linear weiter, das heißt wir lassen wieder die Restriktionen langsam abbauen, also das ist der lineare Anstieg, bis dass wir wieder sehen, es funktioniert nicht, dann fallen wir wieder entsprechend zurück. Also wir sind jetzt in dem Fall bei Corona noch beim zweiten linearen Anstieg, den wir hier gehabt haben, beziehungsweise wir sind gespannt, ob wir denn noch einmal diesen Stauzustand erreichen, beziehungsweise eben bei Corona eine mögliche Überlastung des Krankheitssystems haben. Gut, jetzt kommen wir zum letzten Punkt, was natürlich jetzt noch interessant ist, wo bewegen wir uns denn in der Auslastung des Netzes, wenn wir ein derartiges Verhalten haben? Das heißt, was uns interessiert ist, wie sieht denn der Durchsatz eigentlich aus bei TCP? Und da können wir jetzt natürlich sagen, okay, wir haben den aktuellen Wert des Congestion-Windows und wir haben den Wert, den das Congestion-Window erreicht hat beim Paketverlust. Wir haben dann gleichzeitig das Round-Trip-Delay, das heißt die Rundlaufzeit, die wir entspannen, die wir entspannen, die wir entspannen, die wir entspannen, die wir entspannen, die wir entspannen, die wir entspannen. Und wir nehmen an, dass wir immer dann in den Paketverlust reinlaufen bei einer konstanten Stelle. Das heißt, das Maximum ist irgendwie gegeben durch den Wert W, wenn wir den Paketverlust erreichen. Dann wissen wir, dass der Durchsatz immer pendelt zwischen dem W dividiert durch 2 RCD, also die zweifache Rundlaufzeit und dem W RCD. Klar, das ist das, was wir entsprechend kriegen. Das heißt, entweder fallen wir raus und gehen auf die Hälfte des Congestion-Windows. Das haben wir gerade auf der vorigen Folie entsprechend gehabt. Und in dem Bereich pendelt der Durchsatz. Also diese Sägezahnkurve, die wir vorher gesehen haben, liegt halt immer irgendwo in diesem Bereich drinnen. Und dann kann man relativ einfach ausrechnen, dass der ungefähre Durchsatz bei TCP irgendwo zwischen der Hälfte von W und W liegt, also bei 0,5. Und das ist natürlich ein schönes Beispiel, wo wir sehen, kriegen wir schön hin und wir nützen das Netz nicht vollständig aus, sondern wir sind immer, auch wenn wir konstant Verkehr erzeugen, irgendwo bei 75 Prozent. Und das ist eigentlich ein ganz guter Indikator, wie viel kriegt man eigentlich raus, ohne entsprechend Stau zu erzeugen. Und natürlich geht dieser Wert runter, wenn wir jetzt mehr Verkehrsteilnehmer entsprechend erzeugen. Also damit sind wir jetzt am Ende angelangt von diesem Kapitel 3, Transportschicht. Wir haben uns einiges angeschaut und haben jetzt auch die ganzen Aspekte kennengelernt, mit denen wir eine Transportschicht realisieren können. Was Sie hier sehen, ist ein Update von unserem Mind-Map. Wir haben dann im Nachgang, nach der Transportschicht, vom sehr guten der Erstellung der Folien das noch einmal verfeinert und haben hier noch ein paar zusätzliche Punkte eingetragen, die wir vorher bei der Planung so nicht drinnen hatten. Das grundsätzliche Konzept ist das gleiche, aber wir haben hier ein bisschen mehr Informationen. Das heißt, Sie erinnern sich, wir haben begonnen mit dem netzonabhängigen Transport zwischen den Endsystemen, das ist das, wofür wir die Transportschicht benötigen. Wir haben uns angeschaut, dieses Ende-zu-Ende-Argument, das heißt, wir sehen jetzt wirklich nur das, was auf der Transportschicht ist und haben das Schichten drunter praktisch ausgeblendet. Was brauchen wir, dass wir Nachrichten zwischen Sender und Empfänger hin und her senden können? Und wir haben zwei Ansätze kennengelernt, den verbindungslosen, wo es uns mehr oder weniger egal ist, was mit den Paketen passiert. Wir schicken die auf die Reise und hoffen, dass die irgendwo ankommen. Das ist ein relativ schnelles Verfahren, weil wir uns keine großen Gedanken darüber machen, weil uns eben Zuverlässigkeit in dem Fall nicht so wichtig ist. Und dann umgekehrt eben das verbindungsorientierte Verfahren, wo wir gesagt haben, wir wollen, dass die Pakete sicher ankommen. Wir wollen uns auch in unsicheren Netzen, in unsichereren Transitnetzen irgendwie versichern, dass das, was wir rüberschicken, wirklich auch bei der anderen Seite richtig ankommt, dass es überhaupt ankommt und dass es entsprechend auch in der richtigen Reihenfolge dort ist. Und dafür haben wir verbindungsorientiert eben den Verbindungsaufbau und Abbau kennengelernt, mit dem Drei-Wege-Handschlagsverfahren, also auch dort, damit wir zuverlässig und sicher sind, wann der Aufbau jetzt erledigt ist, beziehungsweise den beidseitigen Verbindungsabbau. Wir haben uns angeschaut, die Sequenznummern und die Quittungen als ganz einfache Mechanismen, die uns sehr viel helfen, einfach um die Segmente durchzunummerieren und um Bestätigungen für den erfolgreichen Erhalt der Nachricht zu haben. Das war so der erste Teil. Die abstrakten Nachrichten sind dann auch noch ganz einfach. Das sind die abstrakten Ansätze, die wir brauchen und die man in verschiedensten Umsetzungen dann verwenden kann. Und wir haben uns dann angeschaut, wie nutzen wir denn das in der Praxis. Und für die Nutzung haben wir gesagt, wir brauchen als erstes noch die Ports, um gewisse Dienste auf der gegenüberliegenden Seite adressieren zu können, das heißt, wir brauchen Ports, um jetzt auswählen zu können, welchen Dienst, welches Protokoll wir entsprechend verwenden. Wir haben gesagt, wir haben die Sockets als Schnittstellenbeschreibung. Als der Weg, wo wir die Nachricht an das Betriebssystem übergeben und der Socket nimmt es dann und baut daraus etwas, was eben auf die andere Seite geschickt wird. Und wir haben als Programmierparadigma den Client-Server-Ansatz gehabt, wo wir gesagt haben, wir haben immer einen Server, der irgendwo läuft und auf irgendetwas wartet und wenn er ein Signal bekommt von einem Client, dann passiert eben die Kommunikation. Und dann haben wir uns diese Verfahren auch in der Praxis angeschaut, indem wir die zwei Hauptprotokolle auf der Transportschicht hergenommen haben, nämlich das UDP und das TCP und haben uns angeschaut, wie die festgelegt sind und da war eben ganz wichtig, dass man nicht nur die jeweiligen Protokollnamen oder Eigenschaften kennenlernt, sondern eben auch, wie dann jetzt das Protokoll-Control-Interface aufgebaut ist, also wie der Header die Information, die ich brauche, um jetzt dieses Protokoll anzuwenden, was steckt da drinnen, weil wir eben hier auch ganz einfach die Protokoll-Interface aufbauen können. Das heißt, wie der Header die Information, die ich brauche, um jetzt dieses Protokoll anzuwenden, was steckt da drinnen, weil wir eben hier auch gewisse Parameter haben, die dieses Protokoll beeinflussen. Und damit haben wir die Grundlagen eigentlich gehabt von dieser Transportschicht und dann sind wir noch ein bisschen an die Verbesserung gegangen. Verbesserung in dem Sinn, dass wir gesagt haben, okay, das normale Verhalten funktioniert in dem Sinne, aber wir möchten was besser machen, wir möchten vielleicht Puffer einführen und die haben wir kennengelernt mit der Fenstertechnik, wie gesagt eine allgemeine Technik, die uns die Möglichkeit gibt, jetzt den Verkehr zu steuern. Und über die Fenster auch gewisse Puffermechanismen einzubauen. Wir haben das dann umgesetzt in der Flusskontrolle, wo wir gesagt haben, der Empfänger steuert, wie viel jetzt vom Sender überhaupt auf den Weg gebracht werden kann, sodass der Empfänger jederzeit in der Lage ist, das abzuarbeiten. Und wenn er mal nicht mehr kann, dann hat er über das Empfangsfenster die Möglichkeit, die nächste Nachricht praktisch aufzuhalten. Und dann haben wir noch zum Schluss die Staukontrolle kennengelernt, wo wir gesagt haben, unser Ziel ist immer, so viel wie möglich ins Netz zu schicken, aber gleichzeitig auch sicherzustellen, dass das Netz nicht überlastet wird. Deswegen haben wir uns da einige Verfahren angeschaut, wie das möglich ist, wie wir überhaupt auf der Schicht Transport feststellen können, dass jetzt ein Stau entsprechend vorliegt. Und das ist jetzt das Bild, das wir von unserer Mindmap entsprechend haben. Und wir haben gesagt, damit haben wir wieder einen Block erledigt. Dieses Kapitel 3. gibt uns jetzt diese Transportschicht und die können wir jetzt zur Seite legen. Das heißt, zu dem Zeitpunkt haben wir verstanden, wie die Transportschicht funktioniert und wir werden uns jetzt als nächstes dann die Vermittlungsschicht anschauen. Das ist die Schicht darunter, an die wir jetzt Nachrichten übergeben und wenn die in der Vermittlungsschicht sind, dann geht es darum, dass wir den Weg durch die Transitnetze zum Empfänger entsprechend beschreiten. Die Mindmap in der aktualisierten Form steht im Moodle wieder bereit. Sie können sich das herunterladen. Wir werden zu dem Zeitpunkt die alten Folien nicht aktualisieren, das hat ja jeweils noch gepasst, aber es ist durchaus vorstellbar, dass man auch jetzt bei der Vermittlungsschicht wieder ein paar Optimierungen entsprechend reinkriegen. Wichtig zu dem Zeitpunkt ist, wir haben unser einfaches Beispiel, Alice schickt eine Nachricht an Bob über das Internet und in der Transportschicht ist es eben um diese virtuelle zuverlässige Verbindung aus Sicht der Endsysteme gegangen. Wo wir praktisch über unser Protokoll zwischen den beiden Seiten kommunizieren und wo wir eben gewisse Mechanismen, gewisse Funktionalität zur Verfügung stellen. Damit sind wir mit den Betrachtungen der Transportschicht fertig. Es folgen jetzt noch eine Reihe von Fragen und weil das in unserem Forum aufgekommen ist, wie die Antworten zu diesen Fragen sind. Normalerweise wäre es so, dass wir praktisch am Ende der Vorlesung diese Fragen immer gestellt haben und beim nächsten Mal, am Beginn der Vorlesung, sind wir kurz durchgegangen, wie denn die Antworten auf die Fragen sein könnten. Jetzt haben wir leider eben keine Präsenzvorlesung zur Zeit, das heißt die Fragen sind immer noch sinnvoll in dem, um den Zweck zu erfüllen, dass Sie sich Gedanken machen, könnte ich das beantworten? Also erste Frage, welche Parameter beeinflussen die Größe des Sequenznummern? Wie haben wir denn festgelegt, wie groß dieser sein muss und wovon hängt es ab, dass wir die Größe entsprechend festlegen? Die Empfehlung ist, Sie nehmen jetzt eine Frage wie hier die erste her und denken einmal darüber nach, ob Sie das jetzt beantworten könnten mit den Informationen, die Sie haben. Und im zweiten Schritt würde ich dann sagen, wenn Ihnen das nicht gelingt, dann nutzen Sie eben das Forum auf der Moodle-Webseite. Oder sprechen gerne am Internet. Da kommen ja auch die Tutoren an, die sind sicher auch in der Lage, alle diese Fragen zu beantworten. Die Fragen sind also etwas für Sie, um Selbstsicherheit zu gewinnen, beziehungsweise noch einmal darüber nachzudenken, ob der Stoff jetzt verständlich war entsprechend und es gibt Ihnen einen Indikator, wie weit Sie sind, wie gut Sie entsprechend sind. Wenn der Wunsch besteht, dass wir das dann durchgehen, dann können wir das am Ende in der Fragestunde machen. Wenn es einmal zu viele Fragen dazu gibt, dann würde man sicher auch einen Termin finden, wo wir uns einmal in einer Videokonferenz treffen und die Fragen entsprechend am Weg bis zum Ende des Semesters durchbesprechen. Gehen wir kurz durch die Fragen. Zweite Frage ist, glaube ich, auch klar. Wie wird die Eindeutigkeit von Sequenznummern fest sichergestellt? Also genau diese Frage auch, wo wir gesagt haben, wie stellen wir denn sicher, dass, wenn der Sequenznummernraum zu Ende ist, dass wir trotzdem immer noch genug Sequenznummern haben. Wozu wird die Fenstertechnik eingesetzt? Da haben wir ja auch heute ein Beispiel kennengelernt. Wie ist es, wenn die Fenster zu klein sind? Kann man sich auch relativ klar vorstellen, was dann entsprechend passiert und wo entsprechend hier die Schwierigkeit ist. Und natürlich auch umgekehrt, wie wirken sich denn zu kleine Sequenznummernräume aus? Also auch hier wieder etwas, wo man sehen kann, wie das zusammenhängt. Die Fragen gehen weiter. Also eine Frage. Ich habe eine Frage. Ich glaube, das ist hoffentlich auch klar. Warum braucht man den 3-Wege-Handschlag? Warum geht es mit dem 2-Wege-Handschlag nicht, um einen sicheren Verbindungsaufbau zu machen? Welche Voraussetzungen, Frage 7, müssten denn gegeben sein, damit der 2-Wege-Handschlag auch sicher funktioniert? Also auch das haben wir einfach von der anderen Seite gedacht. Wir haben ja einen Grund gehabt, warum wir den 3-Wege-Handschlag eingeführt haben. Aber gibt es vielleicht Eigenschaften, die auch ausreichen würden? Dass wir den 2-Wege-Handschlag einsetzen? Warum muss der Verbindungsabbau von beiden Seiten entsprechend passieren? Und die Frage 9. Wo ist denn der große Unterschied zwischen Flusskontrolle und Staukontrolle? Frage 10. Welche Anforderungen einer Anwendung können den Einsatz von UDP rechtfertigen? Das heißt, wofür ist eigentlich UDP ganz gut geeignet? Und warum braucht man das in der Praxis für die eine oder andere Anwendung? Was gibt es da für Beispiele? Wie kann man denn das machen? Dass man welche Anwendungen hat? Wenn wir auch UDP haben? Also wir haben gesagt, UDP ist ja unzuverlässig in dem Sinn. Wie könnten wir denn von der Anwendung her immer noch einen zuverlässigen Datentransfer garantieren, auch wenn wir auf der Transportschicht UDP verwenden? Also das ist etwas, wo man einfach mal drüber nachdenken muss und dann vielleicht sofort versteht, warum dieses Schichtenkonzept so gut ist. Also wir könnten ja auch überlegen, ob zum Beispiel eine TCP-Schicht auf einer UDP-Schicht drauf funktionieren könnte. Wir haben dann Frage 12. Die Frage, wo ELIS zwei TCP-Segmente mit Sequenznummer 90 und 110 an Bob sendet. Die Fragen dazu, wie viele Daten enthält das erste Segment? Das ist glaube ich einfach zum Rechnen hier in dem Fall. Wie lange ist das erste Segment insgesamt? Das deutet schon darauf hin, dass wir vielleicht ein bisschen unterschiedliche Dinge in der PDU unterscheiden müssen. Und auf der anderen Seite. Wie lautet die Quittungsnummer der Bestätigung von Bob, wenn nur das erste Segment verloren geht? Das heißt, was würde Bob entsprechend zurückschicken, wenn er das zweite Paket bekommt, aber das erste Segment nie angekommen ist? Und die letzten vier Fragen. Welche Mechanismen verwendet TCP für eine zuverlässige Ende-zu-Ende-Verbindung? Klar, das haben wir jetzt praktisch zur Genüge besprochen. Wie realisiert man dieses Multiplexen von Anwendungen auf der Transportschicht? Was ist da der wichtige Parameter? Was ist da die wichtige Eigenschaft, die wir eingeführt haben? Dann sollten Sie natürlich wissen, wie das TCP Tahoe funktioniert zur Staukontrolle. Beziehungsweise auch, welche Verbesserungen dann das TCP Reno entsprechend einfügt. Und ganz einfache Frage. Können die Sende- und Empfangs-Sendster bei der Fenstertechnik unterschiedlich groß sein? Also. Sie kriegen schon ein Gefühl dafür. Was wir im Endeffekt bei der Prüfung dann erwarten. Zu dem Punkt ist vielleicht zu sagen, wir diskutieren immer noch, wie wir denn die Prüfung in der aktuellen Zeit durchführen können. Wir haben ein paar Ideen. Die werden wir jetzt noch mit dem Prüfungsausschuss entsprechend diskutieren und schauen, ob wir das durchsetzen können. Und wir hoffen, dass wir dann in den nächsten Tagen, spätestens in den nächsten Wochen Ihnen mitteilen können, wie wir denn entsprechend prüfen werden. Dabei danke ich mich auch für Ihre Geduld. Es ist schwierig und die wirklich einfache Lösung gibt es hier nicht. Mit dem letzten Satz sind wir auch am Ende angelangt. Kapitel 3 ist abgeschlossen. Wir wissen jetzt, wie die Transportschicht funktioniert. Und mit dem nächsten Teil schauen wir uns dann die Vermittlungsschicht an. Ich wünsche Ihnen noch, bleiben Sie gesund und bis demnächst. Danke sehr.ha

# Abschlussfragen

Wie würdest du das beantworten? Vorschläge? Genau, das ist technisch richtig. Warum wollen Sie das überhaupt tun? Genau, zum Beispiel um den Durchsatz zu erhöhen. Wir haben gesehen, was können wir mit der Fenstertechnik noch machen? Genau, wir können auch noch das regulieren, was wir gesagt haben, mit Staukontrolle, Fußkontrolle, diese Geschichte. Was ist das Problem bei zu kleinen Fenstern? Genau das Gegenteil, was wir jetzt gehabt haben. Wenn wir es zu klein machen, dann haben wir natürlich das Problem, dass wir sehr viel liegen lassen, was wir an Kapazität haben. Umgekehrt, wie wir gerade gesehen haben, haben wir das Problem, dass wir sehr viel liegen lassen, wirken sich zu kleine Sequenznummernräume aus. Da könnten Fehler passieren, eben genau durch das, dass vielleicht Nachrichten unterwegs sind, die die gleiche Sequenznummer haben. Also es sind diese Geschichten. Und die Hoffnung ist, wenn ihr euch diese Fragen überlegt, dass ihr durch diese Zusammenhänge, ihr müsst ja immer, die Prüfung ist immer die Überprüfung eurer Transferleistung. Also die Fragen, die jetzt hier stehen, sind jetzt nicht unbedingt die Fragen, die wir jetzt genauso wie auf den Folien gehabt haben, sondern ihr müsst jetzt durch das Verständnis diese Fragen beantworten können. Das habt ihr jetzt hier schon mal ganz gut gemacht. Gut, ups, jetzt ist er in die falsche Richtung gehüpft. Nächster Teil. Warum ist für einen sicheren Verbindungsaufbau auf der Transportschicht ein Dreiwegehandschlag erforderlich? Wer kann sich noch erinnern? Genau, ohne den Dreiwegehandschlag sind wir uns nicht sicher, ob wirklich beide Seiten über das informiert sind, dass jetzt dieser Weg hergestellt ist. Haben wir gesagt, auf Dreiwege könnt ihr immer noch Probleme liefern. Warum ist Dreiwege trotzdem gut? Weil es halt besser ist als Zweiwege. Also das ist auch wieder, beim Internet muss man sehr oft hergehen, es war sehr pragmatisch. Irgendwann muss ich jetzt zu einer Lösung kommen. Ich kann natürlich jetzt Vierwege machen oder Fünfwege theoretisch. Das kann ich beliebig hochspielen. Aber die Verbesserung, die ich daraus kriege, ist so gering, dass wir gesagt haben, das Gute, das Mittel der Wahl ist der Dreiwegehandschlag. Unter welchen Voraussetzungen genügt ein Zweiwegehandschlag für einen Verbindungsaufbau? Jetzt muss man wieder Transferleistungen, die man überlegen muss. Warum den Dreiwege haben wir gemacht? Weil wir einen Fehler vermeiden wollen. Warum kann man also Zweiwege verwenden? Wann reicht denn der Zweiwegehandschlag? Alles gut. Wenn man in eine Richtung, dann reicht es, wenn der eine das weiß. Ja, aber das war gar nicht die Frage, sondern wir haben ja den Dreiwegehandschlag gemacht, weil wir gesagt haben, es könnte ja sein, dass der Zweite die Rückantwort verloren gegangen ist. Oder? Wann geht denn die Rückantwort nicht verloren, müsste ich dann eigentlich die Frage stellen. Oder umgekehrt, warum machen wir denn überhaupt diesen sicheren Verbindungsaufbau? Haben wir ja gesagt. Da haben wir einen Grund dafür gehören. Genau. Und jetzt drehen wir das um. Ihre Antwort drehen wir jetzt bitte um und überlegen uns, wann reicht der Zweiwegehandschlag auch? Genau. Wenn die Verbindungsaufbau hundertprozentig sicher ist, das heißt, ich habe keine unsichere Verbindung, über die ich kommuniziere, sondern die ist sicher, dann reicht der Zweiwege. Dann habe ich das auf jeden Fall gekriegt, dann weiß ich, dass das angekommen ist. Gibt es so etwas?

Ja, man kann sowas schon bauen. Es gibt Netze, die das durch irgendwelche anderen Methoden entsprechend abdecken. Gut, warum muss der Verbindungsapparat beidseitig geschehen? Haben wir erwähnt. Ich glaube, das ist zu trivial, dass man da jetzt viel drüber nachdenkt. Ganz klar. Bitte. Genau. Beim Telefonat ist das auch oft so. Da ist vielleicht die eine Seite schon längst fertig, während die andere noch weiterreden will. Da hilft es mit dem Verbindungsapparat nicht. Was ist der Unterschied zwischen Flusskontrolle und Staukontrolle? Das ist eine gute Frage. Ich habe es heute schon erklärt. Vielleicht einmal jemand anderer. Wer möchte es probieren? Was ist der Unterschied zwischen Flusskontrolle und Staukontrolle? Niemand? Ihr deprimiert mich. Ist es unklar oder traut man sich einfach nichts sagen? Ich weiß es nicht. Wenn es unklar ist, erkläre ich es gerne noch einmal. Aber das ist schon eine wichtige Frage in dem Sinn. Dann sagen Sie es noch einmal. Ja, das ist jetzt sinnvoll aufzulassen. Auflasten. Staukontrolle. Staukontrolle kümmert sich darum, dass wir eben das Netz nicht überraten. Ja, es ist nicht falsch, aber es ist im Prinzip noch einfacher zu sagen. Flusskontrolle kümmert sich um die Datenübertragung zwischen Sender und Empfänger. Zwischen den beiden Punkten. Und Staukontrolle kümmert sich darum, dass es über die Zwischenpunkte, dass es dort eben auch keine Probleme gibt. Ich schließe jetzt. Das ist das eine, das andere aus. Heißt das, wenn ich nur Flusskontrolle mache, dann habe ich keine Staukontrolle? Oder muss ich eins machen und das andere nicht oder so? Sie schütteln meinen Kopf. Also ich interpretiere es als nein, schließe es natürlich nicht aus, sondern ich mache oft beide Sachen oder ich mache meistens beide Sachen. Ich will, dass das zügig zum Empfänger geht. Ich will aber auch, dass das Staus vermeidet, die am Weg irgendwo drinnen sind. Und es ist wirklich. Ich kann nur noch einmal sagen, dieses Verständnis müssen wir dafür haben. Also es ist nicht schwierig. Bitte, gerne. Also bei der Flusskontrolle geht es um den Datenfluss vom Sender zum Empfänger. Wie stelle ich sicher, dass der Sender im Prinzip den Empfänger nicht überlastet? Ja, nehmen wir ein einfaches Beispiel. Ich habe hier einen Supercomputer bei A, Alice ist ein Supercomputer. Der schickt massiv Daten und auf der anderen Seite ist mein Mobiltelefon. Das gehört. Dass der Sender gar nicht so viel Daten empfangen kann. Dann kümmert sich die Flusskontrolle darum, dass der Sender, der Supercomputer gar nicht so viel sendet, dass der Empfänger überlastet ist. Das ist dieser Fluss, dieser Abfluss, dieser Datenfluss. Und bei der Staukontrolle ist es im Prinzip so, dass ich zwei Daten wegstelle. Es ist im Prinzip unerheblich, wie stark das die beiden senden. Wenn die beiden sich jetzt das ausgemacht haben, ich überlaste denen drüben nicht, kann immer noch ein Stau passieren. Der Stau ergibt sich, weil an den Punkten dazwischen möglicherweise irgendjemand anderer das Netz gerade benötigt. Ja? Und die Staukontrolle kümmert sich alles darum, dass man dazwischen Staus entsprechend vermeidet. Hat das geholfen? Prima, danke. Und danke auch fürs Nachfragen. Ich verstehe, dass manchmal die Begriffe schwierig sind, aber deswegen packe ich so, dass ich das nicht mehr kann. Ich weiß nicht, wo drauf rum. Also es sind wirklich zwei fundamentale Unterschiede, die aber relativ ähnlich mit Fenstertechnik gelöst werden. Gut. Nächste Fragen. Welche Anforderungen in einer Anwendung können den Einsatz von UDP rechtfertigen?

Rechtfertigen. Haben wir schon gesagt, UDP ist unsicher, aber schnell. Und daraus ergibt sich, Anwendungen, die schnelle Datenübertragung brauchen, bei denen es aber nicht so wichtig ist, ob es sicher ist, setzen auf UDP. Also zum Beispiel, wenn ich sage, Video, wie gesagt, da könnte ich was verlieren. Ein anderes Beispiel ist, wenn ich so einen Status übertrage und irgendein Lebenszeichen gebe, und da könnte ich sagen, okay, ich übertrage jede Stunde irgendwie ein Lebenszeichen. Wenn dann eine Stunde mal eins ausfällt, weil es verloren gegangen ist, ist das nicht tragisch. Dann warte ich halt auf das Nächste oder sowas. Oder, weiß ich nicht, ich sage, ich brauche ein Lebenszeichen alle Stunde und übertrage alle 15 Minuten eins. Dann ist es vielleicht nicht so tragisch, ob einmal eines fehlt. Also das ist der Einsatzpunkt. Wenn ich sage, UDP will etwas schnell übertragen, oder es ist nicht so tragisch, wenn was fehlt. Dann muss es verloren gehen. Wie kann man in einer Anwendung, die UDP verwendet, einen zuverlässigen Datentransfer garantieren? Das widerspreche ich mir sozusagen. Die zweite Frage scheint ja logisch dem zu widersprechen, was ich gerade bei der ersten Antwort gegeben habe, oder? Redundanzenden ist eine Möglichkeit, aber ich habe immer noch keine Sicherheit. Also ich habe nur eine Verbesserung der Situation. Also Redundanzenden heißt in dem Fall, wenn ich das richtig verstanden habe, sie schicken eine Nachricht und schicken sie es einfach noch einmal und dann hoffen sie, dass eine von den zwei durchkommt, ja? Ja. Besseres Kabel investieren, das ist eine Lösung auf den Schichten drunter. Ja, ist vielleicht die teuerste Lösung, die man jetzt hier machen könnte. Seine ist ein bisschen leichter. Da verlieren wir nur die Hälfte der Bandbreite. Da verlieren wir, oder mindestens die Hälfte der Bandbreite, da verlieren wir ein bisschen mehr Geld, ja? Aber denkt wieder mal an die Schichten. Genau, wie würden Sie das machen in der Schicht darüber? Sie könnten einfach Sequenznummern weiter umnehmen, ja richtig, genau. Also ich kann natürlich genauso mit UDP hier senden und sage, ich kümmere mich um die Zuverlässigkeit in der Schicht drüber, indem ich da einfach meine eigenen Sequenznummern einbaue, ja? Das heißt, ich gehe von oben her über unsere Schnittstelle praktisch eine Sequenznummer mit in den Nutzerdaten und überprüfe auf der Gegenseite die Sequenznummern und wenn das nicht geht, schicke ich einfach die Nachrichten noch einmal, ja? Also, war wunderbar, ja? Genau das ist die richtige Denke, ja? Wie nutzt man das aus? Gut. Alice sendet zwei TCP-Segmente mit Sequenznummern, Sequenznummer 90 und 110 am POP. Wie viele Daten enthält das erste Segment? Wie lange ist das erste Segment insgesamt? Also, wer möchte mitraten? Sie schauen aus, wie wir uns raten würden. 20? Ja, genau. Und wie lange ist das Segment? Ist es dann 20? Genau, richtig, sehr gut, ja? Und jetzt ist die Frage, wie groß ist denn der Header? Haben wir jetzt nicht mehr im Kopf, oder? Müssen wir auf der Folie nachschauen. Okay, aber es ist völlig richtig. Also, wir sehen jetzt hier, wie viele Daten enthält das erste Segment. Wir haben gesagt, die Segmentnummern zählen die Bytes entsprechend mit. Das heißt, wir haben hier 20, 110, wenn er 90 ist, dann diese 20. Das heißt, wir wissen, wir haben 20 Bytes. Aber, und jetzt kommt der zweite Teil der Frage, wie lange ist das Segment? Wie lange ist das erste Segment insgesamt? Wir haben ja immer noch den Header dabei. Der Header, der ja von zum Beispiel der oberen Schicht kommt, oder der jetzt auf dieser Schicht vielleicht noch erweitert worden ist. Tatsächlich nehmen wir den Header auf dieser Schicht. Und der Header auf dieser Schicht, wie bestimmt sich der? Wo ist denn der Schlüssel in der Fragestellung?

um den Header zu bestimmen, um die richtige Folie herauszusuchen, wo dann draufsteht, wie groß der Header ist. Und das sehen wir, der Schlüssel ist das Wort TCP. Alice sendet zwei TCP-Segmente. Das heißt, ich weiß, dass ich von TCP spreche. Das heißt, ich kann bei meinem Protokoll nachschauen, wie groß ist der Header bei TCP. Und wenn man jetzt die Folie noch im Kopf hätte oder sich rausgesucht hätte, dann würde man sehen, der ist mindestens 40 Bytes lang. Wir haben dann unten nur die Optionen, das heißt, der kann länger sein. Das heißt, wir haben jetzt hier noch einmal den Header dabei, der das entsprechend drinnen hat. Den müssen wir jetzt noch dazuzählen. Die 20 Bytes. Das heißt, wir haben 40 Bytes in der Summe. Das heißt, der Header ist so groß wie die Nutztaten hier. Das wollte ich nämlich raus. Okay? Und das muss man sich überlegen. Gut. Wie lautet die Quittungsnummer der Bestätigung von Bob, wenn nur das erste Segment verloren geht? Also, was bestätigt der Bob? Der bestätigt dann das Zweite. Was kommt dann da zurück? Wieder, ihr könnt es tippen. Genau, 90 kommt zurück. Warum kommt 90 zurück? Genau, wir wollen ja dieses 90er-Paket noch einmal wiederholen. Die Aussage treffen wir dann. Das heißt, wir kriegen bestätigt, es ist ein Paket angekommen, aber wir warten immer noch auf das 90er, sozusagen. Also, das ist die Aussage, die dahintersteckt. Gut. Welche Mechanismen verwendet TCP für eine zuverlässige Ende-zu-Ende-Verbindung? Ich kann es nicht mehr hören, aber es ist natürlich der Verbindungsaufbau und die Sequenz nochmal. Das musst du jetzt selbst beantworten. Die nächste Frage ist ein bisschen schwieriger. Wie realisiert man Multiplexen von Anwendungen auf der Transportschicht? Jetzt müssen wir wieder verstehen, was verstehen wir unter Multiplexen und von Anwendungen auf der Transportschicht? Wir denken darüber nach, was könnte das sein? Das ist natürlich dazwischen den Alice und Bob natürlich beliebig viele Anwendungen laufen. Da läuft es nicht. Das ist nicht der WhatsApp oder Signal oder so irgendwas, sondern da läuft vielleicht gleichzeitig nur ein Streaming-Dienst. Ich lade Daten runter, ich mache eine Videokonferenz und so weiter. Was war der Mechanismus, damit man diese Dinge alle gleichzeitig machen kann? Genau, das sind die Ports, die wir drin haben. Sehr gut. Ja, haben wir jetzt auch gehabt. Wie funktioniert TCP-Tar und zur Stahlkontrolle? Wir haben ja gesagt, das war das mit dem Slow Start, mit dem exponentiellen Wachstum bis zum Limit und dann wieder entsprechend linear weiter. Können Sende- und Empfangsfenster bei der Fenstertechnik unterschiedlich groß sein? 50-50 Chance. Ohne Telefon-Joker. Achso, jetzt habe ich da hinten. Bitte. Und warum? Was könnte der Grund sein, dass die unterschiedlich groß sind? Genau, also das ist genau ein sehr gutes Beispiel, wo ich gesagt habe, wenn ich hier einen Supercomputer habe, dann kann der vielleicht relativ viel Speicher für so ein Fenster zur Verfügung stellen, während das Smartphone auf der anderen Seite gar nicht so viel Speicher hat, dass das so viel zur Verfügung stellen kann. Und da fließt es jetzt wieder zusammen mit dem, was wir gerade gehabt haben, mit der Flusskontrolle, weil man halt diesen Abgleich oder die Fenster dazu verwenden, sozusagen zu kontrollieren oder zu regeln, dass der Datentransfer entsprechend drüber geht. Okay, 16 Beispielfragen. Natürlich ist das jetzt nicht eine einzelne Frage bei der Klausur, sondern das ist vielleicht irgendwo ein Unterpunkt zu einer etwas umständlicheren Frage oder etwas komplexeren Frage. Gut, dann.

<!-- Modal START -->
<div id="myModal" class="modal">
  <div class="modal-content">
    <span id="closeModal" class="close">&times;</span>
    <p class="modal-text">
      If MyUniNotes has been helpful and you’d like to support my efforts, <span class="modal-highlight"> you can contribute with a donation: <a class="modal-dono-link" href="https://paypal.me/myuninotes4u">Donate via PayPal</a> :) </span> Your support will help me continue improving the content, but there is no obligation to donate.
    </p>
    <p class="modal-text">
      <span class="modal-highlight">MyUniNotes is a personal, non-revenue project as I believe in accessible education for everyone.</span> I manage this project alongside my studies, with all materials handwritten by me trying to help others understand challenging concepts.
    </p>
  </div>
</div>

<script>
  // JavaScript to display the modal on page load
  document.addEventListener('DOMContentLoaded', function() {
    // Generate a random number between 1 and 1
    // Wanted it to load with a adjustable probability for every page load but did not work, as DOM is loaded only once. Therefore now loading it every time website is visited and DOM is loaded.
    const randomNumber = Math.floor(Math.random() * 1) + 1; 
    // console.log(randomNumber)
    if (randomNumber === 1) {
      setTimeout(function() {
        const modal = document.getElementById('myModal');
        if (modal) {
          modal.classList.add('show');
        }
      }, 1000); // Adjust the delay as needed

      const closeModal = document.getElementById('closeModal');
      if (closeModal) {
        closeModal.addEventListener('click', function() {
          const modal = document.getElementById('myModal');
          if (modal) {
            modal.classList.remove('show');
          }
        });
      }
    } else {
      // Ensure the modal is hidden if the random number is not 1
      const modal = document.getElementById('myModal');
      if (modal) {
        modal.style.display = 'none';
      }
    }
  });
</script>
<!-- Modal END -->

<!-- DISQUS SCRIPT COMMENT START -->

<!-- DISQUS RECOMMENDATION START -->

<div id="disqus_recommendations"></div>

<script> 
(function() { // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
var d = document, s = d.createElement('script'); // IMPORTANT: Replace EXAMPLE with your forum shortname!
s.src = 'https://myuninotes.disqus.com/recommendations.js'; s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>
Please enable JavaScript to view the 
<a href="https://disqus.com/?ref_noscript" rel="nofollow">
comments powered by Disqus.
</a>
</noscript>

<!-- DISQUS RECOMMENDATION END -->

<hr style="border: none; height: 2px; background: linear-gradient(to right, #f0f0f0, #ccc, #f0f0f0); margin-top: 4rem; margin-bottom: 5rem;">
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://myuninotes.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<!-- DISQUS SCRIPT COMMENT END -->

<!-- Modal START -->
<div id="myModal" class="modal">
  <div class="modal-content">
    <span id="closeModal" class="close">&times;</span>
    <p class="modal-text">
      If MyUniNotes has been helpful and you’d like to support my efforts, <span class="modal-highlight"> you can contribute with a donation: <a class="modal-dono-link" href="https://paypal.me/myuninotes4u">Donate via PayPal</a> :) </span> Your support will help me continue improving the content, but there is no obligation to donate.
    </p>
    <p class="modal-text">
      <span class="modal-highlight">MyUniNotes is a personal, non-revenue project as I believe in accessible education for everyone.</span> I manage this project alongside my studies, with all materials handwritten by me trying to help others understand challenging concepts.
    </p>
  </div>
</div>

<script>
  // JavaScript to display the modal on page load
  document.addEventListener('DOMContentLoaded', function() {
    // Generate a random number between 1 and 1
    // Wanted it to load with a adjustable probability for every page load but did not work, as DOM is loaded only once. Therefore now loading it every time website is visited and DOM is loaded.
    const randomNumber = Math.floor(Math.random() * 1) + 1; 
    // console.log(randomNumber)
    if (randomNumber === 1) {
      setTimeout(function() {
        const modal = document.getElementById('myModal');
        if (modal) {
          modal.classList.add('show');
        }
      }, 1000); // Adjust the delay as needed

      const closeModal = document.getElementById('closeModal');
      if (closeModal) {
        closeModal.addEventListener('click', function() {
          const modal = document.getElementById('myModal');
          if (modal) {
            modal.classList.remove('show');
          }
        });
      }
    } else {
      // Ensure the modal is hidden if the random number is not 1
      const modal = document.getElementById('myModal');
      if (modal) {
        modal.style.display = 'none';
      }
    }
  });
</script>
<!-- Modal END -->

<!-- Modal START -->
<div id="myModal" class="modal">
  <div class="modal-content">
    <span id="closeModal" class="close">&times;</span>
    <p class="modal-text">
      If MyUniNotes has been helpful and you’d like to support my efforts, <span class="modal-highlight"> you can contribute with a donation: <a class="modal-dono-link" href="https://paypal.me/myuninotes4u">Donate via PayPal</a> :) </span> Your support will help me continue improving the content, but there is no obligation to donate.
    </p>
    <p class="modal-text">
      <span class="modal-highlight">MyUniNotes is a personal, non-revenue project as I believe in accessible education for everyone.</span> I manage this project alongside my studies, with all materials handwritten by me trying to help others understand challenging concepts.
    </p>
  </div>
</div>

<script>
  // JavaScript to display the modal on page load
  document.addEventListener('DOMContentLoaded', function() {
    // Generate a random number between 1 and 1
    // Wanted it to load with a adjustable probability for every page load but did not work, as DOM is loaded only once. Therefore now loading it every time website is visited and DOM is loaded.
    const randomNumber = Math.floor(Math.random() * 1) + 1; 
    // console.log(randomNumber)
    if (randomNumber === 1) {
      setTimeout(function() {
        const modal = document.getElementById('myModal');
        if (modal) {
          modal.classList.add('show');
        }
      }, 1000); // Adjust the delay as needed

      const closeModal = document.getElementById('closeModal');
      if (closeModal) {
        closeModal.addEventListener('click', function() {
          const modal = document.getElementById('myModal');
          if (modal) {
            modal.classList.remove('show');
          }
        });
      }
    } else {
      // Ensure the modal is hidden if the random number is not 1
      const modal = document.getElementById('myModal');
      if (modal) {
        modal.style.display = 'none';
      }
    }
  });
</script>
<!-- Modal END -->

<!-- Modal START -->
<div id="myModal" class="modal">
  <div class="modal-content">
    <span id="closeModal" class="close">&times;</span>
    <p class="modal-text">
      If MyUniNotes has been helpful and you’d like to support my efforts, <span class="modal-highlight"> you can contribute with a donation: <a class="modal-dono-link" href="https://paypal.me/myuninotes4u">Donate via PayPal</a> :) </span> Your support will help me continue improving the content, but there is no obligation to donate.
    </p>
    <p class="modal-text">
      <span class="modal-highlight">MyUniNotes is a personal, non-revenue project as I believe in accessible education for everyone.</span> I manage this project alongside my studies, with all materials handwritten by me trying to help others understand challenging concepts.
    </p>
  </div>
</div>

<script>
  // JavaScript to display the modal on page load
  document.addEventListener('DOMContentLoaded', function() {
    // Generate a random number between 1 and 1
    // Wanted it to load with a adjustable probability for every page load but did not work, as DOM is loaded only once. Therefore now loading it every time website is visited and DOM is loaded.
    const randomNumber = Math.floor(Math.random() * 1) + 1; 
    // console.log(randomNumber)
    if (randomNumber === 1) {
      setTimeout(function() {
        const modal = document.getElementById('myModal');
        if (modal) {
          modal.classList.add('show');
        }
      }, 1000); // Adjust the delay as needed

      const closeModal = document.getElementById('closeModal');
      if (closeModal) {
        closeModal.addEventListener('click', function() {
          const modal = document.getElementById('myModal');
          if (modal) {
            modal.classList.remove('show');
          }
        });
      }
    } else {
      // Ensure the modal is hidden if the random number is not 1
      const modal = document.getElementById('myModal');
      if (modal) {
        modal.style.display = 'none';
      }
    }
  });
</script>
<!-- Modal END -->

<!-- Modal START -->
<div id="myModal" class="modal">
  <div class="modal-content">
    <span id="closeModal" class="close">&times;</span>
    <p class="modal-text">
      If MyUniNotes has been helpful and you’d like to support my efforts, <span class="modal-highlight"> you can contribute with a donation: <a class="modal-dono-link" href="https://paypal.me/myuninotes4u">Donate via PayPal</a> :) </span> Your support will help me continue improving the content, but there is no obligation to donate.
    </p>
    <p class="modal-text">
      <span class="modal-highlight">MyUniNotes is a personal, non-revenue project as I believe in accessible education for everyone.</span> I manage this project alongside my studies, with all materials handwritten by me trying to help others understand challenging concepts.
    </p>
  </div>
</div>

<script>
  // JavaScript to display the modal on page load
  document.addEventListener('DOMContentLoaded', function() {
    // Generate a random number between 1 and 1
    // Wanted it to load with a adjustable probability for every page load but did not work, as DOM is loaded only once. Therefore now loading it every time website is visited and DOM is loaded.
    const randomNumber = Math.floor(Math.random() * 1) + 1; 
    // console.log(randomNumber)
    if (randomNumber === 1) {
      setTimeout(function() {
        const modal = document.getElementById('myModal');
        if (modal) {
          modal.classList.add('show');
        }
      }, 1000); // Adjust the delay as needed

      const closeModal = document.getElementById('closeModal');
      if (closeModal) {
        closeModal.addEventListener('click', function() {
          const modal = document.getElementById('myModal');
          if (modal) {
            modal.classList.remove('show');
          }
        });
      }
    } else {
      // Ensure the modal is hidden if the random number is not 1
      const modal = document.getElementById('myModal');
      if (modal) {
        modal.style.display = 'none';
      }
    }
  });
</script>
<!-- Modal END -->

<!-- Modal START -->
<div id="myModal" class="modal">
  <div class="modal-content">
    <span id="closeModal" class="close">&times;</span>
    <p class="modal-text">
      If MyUniNotes has been helpful and you’d like to support my efforts, <span class="modal-highlight"> you can contribute with a donation: <a class="modal-dono-link" href="https://paypal.me/myuninotes4u">Donate via PayPal</a> :) </span> Your support will help me continue improving the content, but there is no obligation to donate.
    </p>
    <p class="modal-text">
      <span class="modal-highlight">MyUniNotes is a personal, non-revenue project as I believe in accessible education for everyone.</span> I manage this project alongside my studies, with all materials handwritten by me trying to help others understand challenging concepts.
    </p>
  </div>
</div>

<script>
  // JavaScript to display the modal on page load
  document.addEventListener('DOMContentLoaded', function() {
    // Generate a random number between 1 and 1
    // Wanted it to load with a adjustable probability for every page load but did not work, as DOM is loaded only once. Therefore now loading it every time website is visited and DOM is loaded.
    const randomNumber = Math.floor(Math.random() * 1) + 1; 
    // console.log(randomNumber)
    if (randomNumber === 1) {
      setTimeout(function() {
        const modal = document.getElementById('myModal');
        if (modal) {
          modal.classList.add('show');
        }
      }, 1000); // Adjust the delay as needed

      const closeModal = document.getElementById('closeModal');
      if (closeModal) {
        closeModal.addEventListener('click', function() {
          const modal = document.getElementById('myModal');
          if (modal) {
            modal.classList.remove('show');
          }
        });
      }
    } else {
      // Ensure the modal is hidden if the random number is not 1
      const modal = document.getElementById('myModal');
      if (modal) {
        modal.style.display = 'none';
      }
    }
  });
</script>
<!-- Modal END -->
