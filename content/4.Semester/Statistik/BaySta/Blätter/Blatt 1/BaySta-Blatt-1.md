---
tags:
  - 4semester
  - BaySta
  - statistik
  - √úbungsblatt
fach: "[[BaySta]]"
Thema:
  - "[[Unterschied zwischen Stetigkeit und Diskretheit]]"
  - "[[Priori und Posteriori Wahrscheinlichkeit]]"
date created: Monday, 22. April 2024, 23:35
date modified: Thursday, 16. May 2024, 16:59
---

# TODO:

- [ ] [[BaySta-Blatt-1#Aufgabe 2#(e) Wie hoch ist die Wahrscheinlichkeit, dass eine Person bei der bei diesem Vorgehen der PCR-Test positiv ist, tats√§chlich Corona-infiziert ist?|BaySta-1-2e nochmal machen]]
- [ ] [[BaySta-Blatt-1#Aufgabe 3|BaySta 1-3 checken]]

# Aufgabe 1

> [!note] Aufgabenstellung
> Eine Schokoladenfabrik stellt Pralinen her, die jeweils eine Kirsche enthalten. Die ben√∂tigten Kirschen werden an zwei Maschinen entkernt. Maschine A liefert 70 % dieser Kirschen, wobei 8 % der von A gelieferten Kirschen den Kern noch enthalten. Maschine B produziert 30 % der ben√∂tigten Kirschen, wobei 5 % der von B gelieferten Kirschen den Kern noch enthalten. Bei einer abschlie√üenden Gewichtskontrolle werden 95 % der Pralinen, in denen ein Kirschkern enthalten ist, aussortiert, aber auch 2 % der Pralinen ohne Kern.

## (a) Modellieren Sie diesen mehrstufigen Vorgang geeignet. Wie gro√ü ist die Wahrscheinlichkeit, dass eine Praline mit Kirschkern in den Verkauf gelangt?

- Maschine A liefert 70% der Kirschen mit Fehlerquote von 8%
- Maschine B liefert 30% der Kirsche mit Fehlerquote von 5%
- Endkontrolle:
  - 95% der Pralinen mit Kern werden korrekt aussortiert
  - 2% der Pralinen ohne Kern werden f√§lschlicherweise aussortiert

Gefragt ist nach: **Praline mit Kirschkern wird nicht aussortiert**

$$
\begin{aligned}
P(\text{"Kirschkern landet im Verkauf"}) &= (0.7\cdot0.08+0.3\cdot0.05) \cdot 0.05\\
&= 0.00355 \\
&\approx 0.36 \space \%
\end{aligned}
$$

## (b) Ein Kunde kauft eine Packung mit 100 Pralinen. Wie gro√ü ist die Wahrscheinlichkeit, dass nur gute Pralinen, also Pralinen ohne Kirschkern, in der Packung sind?

> [!danger] Muss noch aussgebessert werden vergleiche ML

$$
\begin{aligned}
P({\text{"Kein Kirschkern landet im Verkauf"}}) &= 1- P(\text{"Kirschkern landet im Verkauf"})\\
&= 1-(0.7\cdot0.08+0.3\cdot0.05) \cdot 0.05\\
&=1-0.00355 \\
&=0.99645\\
&\approx 99.6 \space \%
\end{aligned}
$$

$$
P (\text{"Nur gute Pralinen in 100 Packungen"})= \frac{0.99645^{100}}{0.99645+0.00355}
$$

# Aufgabe 2

> [!note] Aufgabenstellung
> Nehmen wir an, die Pr√§valenz von Corona an einem gewissen Tag liege bei 20 ansteckenden Personen bei 100.000 Einwohnern.
>
> Die Spezifit√§t (Wahrscheinlichkeit, dass der Test einer gesunden Person negativ ausf√§llt) eines Antigen-Tests liege bei 98%. Die Sensitivit√§t (Wahrscheinlichkeit, dass der Test bei einer erkrankten Person positiv ausf√§llt) des Antigen-Tests liege bei 90%.
>
> Die Spezifit√§t eines PCR-Tests liege bei 99%. Die Sensitivit√§t des PCR-Tests liege bei 98%.
>
> Es sei folgendes Vorgehen √ºblich: Es wird erst ein Antigen-Test durchgef√ºhrt. F√§llt er positiv aus, so wird ein PCR-Test ausgef√ºhrt.

## (a) Formulieren Sie die hier dargestellten Ereignisse und Wahrscheinlichkeiten.

$P(T) = \text{Test positiv}$  
$P(Krank) = P(K)= \frac{20}{100000}=0.0002$

### Antigen-Test:

- $P_{AG}(\overline T | \overline{Krank}) = 0.98$
- $P_{AG}( T | Krank) = 0.90$

### PCR-Test:

- $P_{PCR}(\overline T | \overline{Krank}) = 0.99$
- $P_{PCR}( T | Krank) = 0.98$

## (b) Wie hoch ist die Wahrscheinlichkeit, dass eine Person, bei der auch der PCR-Test positiv ist, tats√§chlich Corona-infiziert ist?

Um die Wahrscheinlichkeit zu bestimmen, dass eine Person tats√§chlich Corona-infiziert ist, nachdem sowohl der Antigen-Test als auch der PCR-Test positiv ausgefallen sind, k√∂nnen wir den Satz von Bayes verwenden. Wir formulieren die erforderlichen Wahrscheinlichkeiten und wenden die Gesetze der Wahrscheinlichkeit an:

### Gegeben:

- $P(K) = \text{Wahrscheinlichkeit, dass eine Person infiziert ist} = 0.0002$
- $P(\overline{K}) = \text{Wahrscheinlichkeit, dass eine Person nicht infiziert ist} = 0.9998$

- $P_{AG}(T | K) = \text{Sensitivit√§t des Antigen-Tests} = 0.90$
- $P_{AG}(\overline{T} | \overline{K}) = \text{Spezifit√§t des Antigen-Tests} = 0.98$

- $P_{PCR}(T | K) = \text{Sensitivit√§t des PCR-Tests} = 0.98$
- $P_{PCR}(\overline{T} | \overline{K}) = \text{Spezifit√§t des PCR-Tests} = 0.99$

### Berechnung der Wahrscheinlichkeiten:

- Wahrscheinlichkeit, dass beide Tests positiv sind, wenn die Person krank ist:

  $$
  P_{AG \cap PCR}(T | K) = P_{AG}(T | K) \times P_{PCR}(T | K) = 0.90 \times 0.98 = 0.882
  $$

- Wahrscheinlichkeit, dass beide Tests positiv sind, wenn die Person nicht krank ist:

  $$
  P_{AG \cap PCR}(T | \overline{K}) = (1 - P_{AG}(\overline{T} | \overline{K})) \times (1 - P_{PCR}(\overline{T} | \overline{K})) = 0.02 \times 0.01 = 0.0002
  $$

- Gesamtwahrscheinlichkeit, dass beide Tests positiv sind:
  $$
  P_{AG \cap PCR}(T) = P_{AG \cap PCR}(T | K) \times P(K) + P_{AG \cap PCR}(T | \overline{K}) \times P(\overline{K}) = 0.882 \times 0.0002 + 0.0002 \times 0.9998 = 0.0003764
  $$

### Anwendung der Bayes-Formel:

$$
P(K | T_{AG \cap PCR}) = \frac{P_{AG \cap PCR}(T | K) \times P(K)}{P_{AG \cap PCR}(T)} = \frac{0.882 \times 0.0002}{0.0003764} \approx 0.468
$$

### Antwort:

Die Wahrscheinlichkeit, dass eine Person tats√§chlich Corona-infiziert ist, nachdem sowohl der Antigen-Test als auch der PCR-Test positiv ausgefallen sind, betr√§gt ungef√§hr $46.8\%$.

## (c) Wie hoch ist die Wahrscheinlichkeit, dass eine infizierte Person nicht erkannt wird?

> [!danger] Aufgepasst!
> Es ist wichtig, den Unterschied zu verstehen: $P(\overline{T} | K)$ gibt die Wahrscheinlichkeit an, dass der Test negativ ist, obwohl die Person infiziert ist. Dies ist relevant, um die Zuverl√§ssigkeit des Tests zu bewerten. Hingegen bedeutet $P(K | \overline{T})$, dass eine Person tats√§chlich infiziert ist, obwohl ihr Test negativ ausgefallen ist. Diese Wahrscheinlichkeit ist wichtig, um das Risiko einer unerkannten Infektion bei einem negativen Testergebnis einzusch√§tzen.

Die Wahrscheinlichkeit, dass der Antigen-Test negativ ist, obwohl die Person infiziert ist (Fehlalarmrate oder Fehler 2. Art), ist das Komplement der Sensitivit√§t des Tests:

- [ ] Baysta-Blatt-1-2c ausbessern
  > [!warning] Der Fall Antigen positiv PCR negativ fehlt

> [!danger] Spezifit√§t benutzen nicht Sensitivit√§t

$$
P_{AG}(\overline{T} | K) = 1 - P_{AG}(T | K) = 1 - 0.90 = 0.10
$$

$$
P_{AG}(\overline{T} | K) = 0.10
$$

Diese Wahrscheinlichkeit gibt an, dass 10¬†% der tats√§chlich infizierten Personen durch den Antigen-Test nicht erkannt werden. Der PCR-Test ist nicht n√∂tig, da dieser bei einem negativen Antigen-test nicht ausgef√ºhrt wird.

---

> [!note] Aufgabenstellung
> Alternativ ist folgendes Vorgehen √ºblich: Es werden nur Personen mit starken Symptomen getestet. Das betrifft etwa 2% der Personen. Wir k√∂nnen annehmen, dass unter diesen etwa 1% der Personen infiziert sind.

> [!warning] Es √§ndert sich nur die Pr√§valenz auf 0.01 ist einfach mies formuliert in der Aufgabenstellung

## (d) Formulieren Sie die hier dargestellten Ereignisse und Wahrscheinlichkeiten.

$$
\begin{aligned}
P(\text{"Starke Symptome"})&=  0.02\\

\end{aligned}
$$

$$
P(\text{"Krank" | "Starke Symptome"})=0.01
$$

## (e) Wie hoch ist die Wahrscheinlichkeit, dass eine Person bei der bei diesem Vorgehen der PCR-Test positiv ist, tats√§chlich Corona-infiziert ist?

#Frage

> [!bug] Sa√ü zu lange an dieser Aufgabe, hab aufgegeben (easily 1.5h+ input) ü™¶
>
> - Welche Schritte wie das ganze angehen?
> - Meine probierte L√∂sung ist unter dem Trennstrich
> - Es fehlt $P(K)$ bzw $P( K | \overline S )$

### Gesucht

Die Wahrscheinlichkeit $P(\text{Krank} | T)$, dass eine Person, die einen positiven PCR-Test hat, tats√§chlich mit Corona infiziert ist, nachdem zuvor ein positiver Antigen-Test vorliegt.

### Gegeben

- $P(\text{"Krank" | "Starke Symptome"}) = 0.01$ (Wahrscheinlichkeit, dass eine Person krank ist, gegeben dass sie starke Symptome zeigt)
- $P(\text{"Starke Symptome"}) = 0.02$ (Wahrscheinlichkeit, dass eine Person starke Symptome zeigt)
- $P_{PCR}(T | \text{Krank}) = 0.98$ (Sensitivit√§t des PCR-Tests)
- $P_{PCR}(\overline{T} | \overline{\text{Krank}}) = 0.99$ (Spezifit√§t des PCR-Tests)
- $P_{AG}(T | \text{Krank}) = 0.90$ (Sensitivit√§t des Antigen-Tests)
- $P_{AG}(T | \overline{\text{Krank}}) = 0.02$ (Wahrscheinlichkeit eines falsch positiven Antigen-Tests, abgeleitet aus der Spezifit√§t von 98%)

### Berechnung

Die Gesamtwahrscheinlichkeit eines positiven PCR-Tests $P(T)$ unter Personen mit einem vorherigen positiven Antigen-Test ist:

$$
P(T) \approx 0.009018\approx \text{(0.9018\%)}
$$

Die Wahrscheinlichkeit, dass eine Person, die einen positiven PCR-Test hat, tats√§chlich mit Corona infiziert ist, gegeben dass zuvor ein Antigen-Test positiv war, betr√§gt:

$$
P(\text{Krank} | T) \approx 0.9780 \approx\text{(97.80\%)}
$$

Dies ber√ºcksichtigt das Vorgehen, bei dem nur Personen mit einem vorherigen positiven Antigen-Test anschlie√üend einen PCR-Test erhalten.

---

### Gesucht:

$$
P_{AG}(K|T) \cap P_{PCR}(K|T)
$$

### Bekannt:

- $P_{AG}(T | K) = \text{Sensitivit√§t des Antigen-Tests} = 0.90$
- $P_{PCR}(T | K) = \text{Sensitivit√§t des PCR-Tests} = 0.98$
- $P(\text{"Starke Symptome"})=P(S)=  0.02$
- $P(\text{"Krank" | "Starke Symptome"})=0.01$

$$
\begin{aligned}
P(K) &= P(K|S) \cdot P(S) + P( K|\overline S) \cdot P(\overline S)\\
&=0.01\cdot0.02+
\end{aligned}
$$

$$
\begin{aligned}
P_{AG}(T) &= P_{AG}(T | K) \cdot P(K) + P_{AG}(\overline{T} | \overline{K}) \cdot P(\overline{K})\\


\end{aligned}
$$

$$
\begin{aligned}
P_{AG}(K\space|\space T) &= \frac{P_{AG}(T | K) \cdot P(K)}{P_{AG}(T)}\\
\\

\\

\\

\end{aligned}
$$

## (f) Wie hoch ist die Wahrscheinlichkeit, dass eine infizierte Person nicht erkannt wird?

Frage:: #Frage Ist das nicht gleich zur Teilaufgabe davor?

### Gesucht:

$$
P(\text{"Test negativ" | "Krank"})=P_{AG}(\overline T|K)
$$

$$
P_{AG}(\overline{T} | K) = 1 - P_{AG}(T | K) = 1 - 0.90 = 0.10
$$

$$
P_{AG}(\overline{T} | K) = 0.10
$$

# Aufgabe 3

## [[Unterschied zwischen Stetigkeit und Diskretheit]]

> [!note] Aufgabenstellung
> Wir betrachten ein Binomial-Experiment. Sei $x \sim B(n, \pi)$. Wir betrachten im Folgenden zwei Ans√§tze:
>
> 1.  $\pi$ ist stetig
> 2.  $\pi$ sei diskret und nehme Werte auf einem Gitter $[0, 0.01, 0.02, ‚Ä¶, 0.99, 1]$ an.

## (a) Ausgehend von Laplace' Prinzip vom unzureichenden Grund: Wie sieht in beiden F√§llen die Priori von $\pi$ aus?

Priori Verteilung bedeutet, dass $\pi$ eine stetige Gleichverteilung ist, die als $U(0,1)$ dargestellt wird

_1.Fall:_

- $\pi$ ist stetig $\rightarrow \frac{1}{b-a}\space f√ºr\space \pi \in [a,b]$
- Da $a=0$ und $b=1$ $\rightarrow f(\pi) = 1 \space f√ºr \space \pi \in[0,1]$
- Das bedeutet, dass die Wahrscheinlichkeit f√ºr jeden Wert von $\pi$ innerhalb dieses Bereichs gleich ist, und die kumulative Verteilungsfunktion (CDF) ist:
  $$
  F(\pi)=\pi \space f√ºr\space x\in[0,1]
  $$
  **Fall 2:**
- $\pi$ ist diskret und nimmt Werte auf einem Gitter $[0, 0.01, 0.02, ‚Ä¶, 0.99, 1]$ an
  $$
  P(\pi = k) = \frac{1}{101} \space f√ºr\space k\in \{0,0.01,0.02,‚Ä¶,0.99,1\}
  $$

> [!info]- Erkl√§rung
> Das obige zeigt eine Aufgabenstellung zur Bayes-Statistik mit Bezug auf das Laplace'sche Prinzip des unzureichenden Grundes. Wir betrachten ein Binomial-Experiment, bei dem die Wahrscheinlichkeit $\pi$ eine unbekannte Gr√∂√üe ist, und es wird eine Priori-Verteilung f√ºr $\pi$ auf Basis dieses Prinzips gesucht. Hier wird zwischen zwei F√§llen unterschieden: einer stetigen und einer diskreten Verteilung von $\pi$.
>
> ### Stetige Gleichverteilung (Fall 1)
>
> In einer stetigen Gleichverteilung $\mathrm{U}(a, b)$ hat jedes Element im Intervall $[a, b]$ die gleiche Wahrscheinlichkeit. Die Dichte $f(\pi)$ wird als konstant $1/(b-a)$ f√ºr alle $\pi$ im Intervall angenommen, da die Fl√§che unter der Dichtefunktion 1 sein muss (was die Gesamtwahrscheinlichkeit repr√§sentiert). F√ºr $a = 0$ und $b = 1$ (wie es im Fall der Binomialverteilung f√ºr eine Wahrscheinlichkeit $\pi$ sinnvoll ist), ist die Dichte einfach $1/(1-0) = 1$. Dies bedeutet, dass jedes $\pi$ im Intervall $[0, 1]$ gleich wahrscheinlich ist.
> Die kumulative Verteilungsfunktion (CDF) gibt die Wahrscheinlichkeit an, dass die Zufallsvariable $\pi$ kleiner oder gleich einem bestimmten Wert ist. F√ºr eine stetige Gleichverteilung ist die CDF einfach $F(\pi) = \pi$, weil sie linear von 0 bis 1 ansteigt.
>
> ### Diskrete Gleichverteilung (Fall 2)
>
> F√ºr eine diskrete Gleichverteilung, bei der die Zufallsvariable $\pi$ nur bestimmte Werte annehmen kann (hier ein Gitter von 0 bis 1 in Schritten von 0.01), wird jedem dieser Punkte die gleiche Wahrscheinlichkeit zugeordnet. Da es 101 solche Punkte gibt, ist die Wahrscheinlichkeit f√ºr jeden Punkt $1/101$.
> In beiden F√§llen geht man davon aus, dass man keine Informationen √ºber $\pi$ hat, au√üer dass es zwischen 0 und 1 liegt. Daher behandelt man alle m√∂glichen Werte von $\pi$ als gleich wahrscheinlich.
> In der Bayes-Statistik werden diese Priori-Verteilungen genutzt, um sie mit den Daten zu kombinieren und so eine Posteriori-Verteilung zu erhalten. Diese reflektiert dann sowohl das Vorwissen (die Priori-Verteilung) als auch die Informationen aus den Daten.

## (b) Leiten Sie jeweils die Posteriori f√ºr $\pi|x$ her. [[Erstellung der Posteriori-Verteilung im Bayes'schen Kontext|(Erstellung der Posteriori-Verteilung im Bayes'schen Kontext)

> [!danger] k auswechseln mit x

> [!success] L√∂sung
> Die Posteriori-Wahrscheinlichkeit $p(\pi|x)$ ist die Wahrscheinlichkeit f√ºr den Parameter $\pi$ gegeben die beobachteten Daten $x$. Um diese zu berechnen, verwenden wir das Bayes'sche Theorem:
>
> $$
> p(\pi|x)=\frac{f(x|\pi)f(\pi)}{f(x)}
> $$
>
> ### Schritte zur Herleitung der Posteriori-Verteilung
>
> 1. **Bestimmung der Likelihood $f(x|\pi)$:**  
>    Die Likelihood-Funktion $f(x|\pi)$ f√ºr ein Binomial-Experiment mit $n$ Versuchen und $k$ Erfolgen ist gegeben durch die Binomialverteilung:
>
>    $$
>    f(x|\pi) = \binom{n}{k} \pi^k (1 - \pi)^{n-k}
>    $$
>
> 2. **Bestimmung der Priori-Verteilung $f(\pi)$:**  
>    Wenn keine vorherige Information √ºber $\pi$ vorliegt, nehmen wir an, dass $\pi$ gleichverteilt ist. Das bedeutet:
>
>    - F√ºr eine stetige Gleichverteilung:
>
>      $$
>      f(\pi) = 1 \text{ f√ºr } 0 \leq \pi \leq 1
>      $$
>
>    - F√ºr eine diskrete Gleichverteilung auf einem Gitter von 0 bis 1 in Schritten von 0.01:
>
>      $$
>      f(\pi) = \frac{1}{101}
>      $$
>
> 3. **Bestimmung der Marginal Likelihood $f(x)$:**  
>    Die Marginal Likelihood $f(x)$ ist die Wahrscheinlichkeit der beobachteten Daten √ºber alle m√∂glichen Werte von $\pi$. Dies ist die Summe (im diskreten Fall) oder das Integral (im stetigen Fall) der Likelihood multipliziert mit der Priori-Verteilung.
>
>    - F√ºr eine stetige Gleichverteilung:
>
>      $$
>      f(x) = \int_0^1 \binom{n}{k} \pi^k (1 - \pi)^{n-k} d\pi
>      $$
>
>    - F√ºr eine diskrete Gleichverteilung:
>
>      $$
>      f(x) = \sum_{\pi \in \{0, 0.01, \ldots, 1\}} \frac{1}{101} \binom{n}{k} \pi^k (1 - \pi)^{n-k}
>      $$
>
> ### Berechnung der Posteriori-Verteilung
>
> Mit der Likelihood, der Priori-Verteilung und der Marginal Likelihood k√∂nnen wir nun die Posteriori-Verteilung $p(\pi|x)$ berechnen. Diese gibt uns die aktualisierte Wahrscheinlichkeit f√ºr $\pi$ nach dem Beobachten der Daten $x$.
>
> - F√ºr eine stetige Gleichverteilung:
>
>   $$
>   p(\pi|x) = \frac{\binom{n}{k} \pi^k (1 - \pi)^{n-k}}{\int_0^1 \binom{n}{k} \pi^k (1 - \pi)^{n-k} d\pi}
>   $$
>
> - F√ºr eine diskrete Gleichverteilung:
>
>   $$
>   p(\pi|x) = \frac{\frac{1}{101} \binom{n}{k} \pi^k (1 - \pi)^{n-k}}{\sum_{\pi \in \{0, 0.01, \ldots, 1\}} \frac{1}{101} \binom{n}{k} \pi^k (1 - \pi)^{n-k}}
>   $$

> [!tip]- Step by Step Merkhilfe
>
> ### Schritt 1: Verstehen des Bayes'schen Theorems
>
> Starten Sie mit dem Bayes'schen Theorem:
>
> $$
> p(\pi|x) = \frac{f(x|\pi)f(\pi)}{f(x)}
> $$
>
> ### Schritt 2: Likelihood-Funktion definieren
>
> Die Likelihood $f(x|\pi)$ f√ºr ein Binomial-Experiment ist:
>
> $$
> f(x|\pi) = \binom{n}{k} \pi^k (1 - \pi)^{n-k}
> $$
>
> Hierbei steht $n$ f√ºr die Gesamtzahl der Versuche, $k$ f√ºr die Anzahl der Erfolge und $\pi$ f√ºr die Erfolgswahrscheinlichkeit.
>
> ### Schritt 3: Priori-Verteilung w√§hlen
>
> Entscheiden Sie sich f√ºr eine angemessene Priori-Verteilung $f(\pi)$. F√ºr eine uniforme Priori:
>
> - Stetig: $f(\pi) = 1$ f√ºr $\pi$ in $[0, 1]$.
> - Diskret: $f(\pi) = \frac{1}{101}$ f√ºr $\pi$ in $\{0, 0.01, \ldots, 1\}$.
>
> ### Schritt 4: Marginal Likelihood bestimmen
>
> Berechnen Sie die Marginal Likelihood $f(x)$:
>
> - Stetig: $f(x) = \int_0^1 \binom{n}{k} \pi^k (1 - \pi)^{n-k} d\pi$.
> - Diskret: $f(x) = \sum_{\pi \in \{0, 0.01, \ldots, 1\}} \frac{1}{101} \binom{n}{k} \pi^k (1 - \pi)^{n-k}$.
>
> ### Schritt 5: Posteriori-Verteilung berechnen
>
> Setzen Sie die Likelihood und die Priori in das Bayes'sche Theorem ein, um die Posteriori-Verteilung zu ermitteln:
>
> - Stetig:
>
> $$
> p(\pi|x) = \frac{\binom{n}{k} \pi^k (1 - \pi)^{n-k}}{\int_0^1 \binom{n}{k} \pi^k (1 - \pi)^{n-k} d\pi}
> $$
>
> - Diskret:
>
> $$
> p(\pi|x) = \frac{\frac{1}{101} \binom{n}{k} \pi^k (1 - \pi)^{n-k}}{\sum_{\pi \in \{0, 0.01, \ldots, 1\}} \frac{1}{101} \binom{n}{k} \pi^k (1 - \pi)^{n-k}}
> $$
>
> ### Schritt 6: Posteriori-Verteilung interpretieren
>
> Verwenden Sie die Posteriori-Verteilung, um aktualisierte Wahrscheinlichkeiten f√ºr $\pi$ zu erhalten, basierend auf den beobachteten Daten $x$.

## (c) Berechnen Sie jeweils den Posteriori-Erwartungswert und den Posteriori-Median f√ºr folgende Daten:

- $n = 10, x = 3$
- $n = 100, x = 13$
- $n = 1000, x = 33$

### $n = 10, X = 3$

> [!success] L√∂sung
>
> # Posteriori-Erwartungswert und -Median f√ºr Binomialdaten
>
> Um den Posteriori-Erwartungswert und den Posteriori-Median f√ºr die gegebenen Daten unter Verwendung der zwei unterschiedlichen Priori-Annahmen zu berechnen, m√ºssen wir zuerst das Bayesianische Update f√ºr die Wahrscheinlichkeit $\pi$ durchf√ºhren, gegeben die Daten $x \sim B(n, \pi)$, wobei $n=10$ und $x=3$. Die Beobachtungen folgen einer Binomialverteilung.
>
> ## Fall 1: Stetige Gleichverteilung von $\pi$ ($\pi \sim U(0, 1)$)
>
> ### Priori
>
> Die Priori-Verteilung von $\pi$ ist $U(0, 1)$, was bedeutet, dass sie eine Beta-Verteilung mit Parametern $\alpha=1$ und $\beta=1$ ist: $\text{Beta}(1, 1)$.
>
> ### Likelihood
>
> Die Likelihood-Funktion f√ºr die Beobachtungen aus einer Binomialverteilung mit den gegebenen Parametern ist proportional zu:
>
> $$
> \pi^x (1-\pi)^{n-x} = \pi^3 (1-\pi)^7
> $$
>
> ### Posteriori
>
> Die Posteriori-Verteilung ist das Produkt von Priori und Likelihood, das ebenfalls eine Beta-Verteilung ergibt:
>
> $$
> \text{Beta}(\alpha + x, \beta + n - x) = \text{Beta}(4, 8)
> $$
>
> #### Berechnungen
>
> Der **Posteriori-Erwartungswert** f√ºr eine Beta-Verteilung $\text{Beta}(a, b)$ ist:
>
> $$
> E[\pi] = \frac{a}{a + b} = \frac{4}{4 + 8} = \frac{4}{12} = \frac{1}{3}
> $$
>
> Der **Posteriori-Median** kann n√§herungsweise durch numerische Methoden berechnet werden, da f√ºr die Beta-Verteilung keine einfache analytische L√∂sung f√ºr den Median existiert. Der Median liegt jedoch nahe dem Erwartungswert.
>
> ## Fall 2: Diskrete Gleichverteilung von $\pi$
>
> ### Priori
>
> In diesem Fall ist $\pi$ diskret verteilt auf dem Gitter $[0, 0.01, 0.02, \dots, 0.99, 1]$, und jeder Wert ist gleich wahrscheinlich.
>
> ### Likelihood
>
> Wie oben.
>
> ### Posteriori
>
> F√ºr jede diskrete Stelle $\pi_k$ in $[0, 0.01, 0.02, \dots, 0.99, 1]$ berechnen wir das Posteriori proportional zu:
>
> $$
> \pi_k^3 (1-\pi_k)^7
> $$
>
> Anschlie√üend normalisieren wir diese Wahrscheinlichkeiten, damit sie sich zu 1 summieren. Die Positionen des h√∂chsten Wertes geben uns den Modus, und wir k√∂nnen die kumulativen Wahrscheinlichkeiten berechnen, um den Median zu finden.
>
> #### Berechnungen
>
> Der **Posteriori-Erwartungswert** f√ºr dieses Gitter kann n√§herungsweise berechnet werden als:
>
> $$
> E[\pi] \approx \sum_{k=0}^{100} \pi_k \cdot P(\pi = \pi_k \mid x=3, n=10)
> $$
>
> Der **Posteriori-Median** wird identifiziert, indem die kumulativen Wahrscheinlichkeiten berechnet werden, bis sie 0.5 erreichen.
>
> F√ºr pr√§zisere numerische Berechnungen, besonders im diskreten Fall, sind Softwaretools wie Python oder R hilfreich, um die Wahrscheinlichkeiten zu berechnen und zu normalisieren.

### $n = 100, X = 13$

> [!success] L√∂sung
>
> # Posteriori-Erwartungswert und -Median f√ºr Binomialdaten
>
> Um den Posteriori-Erwartungswert und den Posteriori-Median f√ºr die gegebenen Daten unter Verwendung der zwei unterschiedlichen Priori-Annahmen zu berechnen, m√ºssen wir zuerst das Bayesianische Update f√ºr die Wahrscheinlichkeit $\pi$ durchf√ºhren, gegeben die Daten $x \sim B(n, \pi)$, wobei $n=100$ und $x=13$. Die Beobachtungen folgen einer Binomialverteilung.
>
> ## Fall 1: Stetige Gleichverteilung von $\pi$ ($\pi \sim U(0, 1)$)
>
> ### Priori
>
> Die Priori-Verteilung von $\pi$ ist $U(0, 1)$, was bedeutet, dass sie eine Beta-Verteilung mit Parametern $\alpha=1$ und $\beta=1$ ist: $\text{Beta}(1, 1)$.
>
> ### Likelihood
>
> Die Likelihood-Funktion f√ºr die Beobachtungen aus einer Binomialverteilung mit den gegebenen Parametern ist proportional zu:
>
> $$
> \pi^x (1-\pi)^{n-x} = \pi^{13} (1-\pi)^{87}
> $$
>
> ### Posteriori
>
> Die Posteriori-Verteilung ist das Produkt von Priori und Likelihood, das ebenfalls eine Beta-Verteilung ergibt:
>
> $$
> \text{Beta}(\alpha + x, \beta + n - x) = \text{Beta}(1+13, 1+100-13) = \text{Beta}(14, 88)
> $$
>
> #### Berechnungen
>
> Der **Posteriori-Erwartungswert** f√ºr eine Beta-Verteilung $\text{Beta}(a, b)$ ist:
>
> $$
> E[\pi] = \frac{a}{a + b} = \frac{14}{14 + 88} = \frac{14}{102} \approx 0.1373
> $$
>
> Der **Posteriori-Median** kann n√§herungsweise durch numerische Methoden berechnet werden, da f√ºr die Beta-Verteilung keine einfache analytische L√∂sung f√ºr den Median existiert. Der Median wird jedoch in der N√§he des Erwartungswertes liegen, leicht niedriger aufgrund der Schiefe der Verteilung.
>
> ## Fall 2: Diskrete Gleichverteilung von $\pi$
>
> ### Priori
>
> In diesem Fall ist $\pi$ diskret verteilt auf dem Gitter $[0, 0.01, 0.02, \dots, 0.99, 1]$, und jeder Wert ist gleich wahrscheinlich.
>
> ### Likelihood
>
> Wie oben.
>
> ### Posteriori
>
> F√ºr jede diskrete Stelle $\pi_k$ in $[0, 0.01, 0.02, \dots, 0.99, 1]$ berechnen wir das Posteriori proportional zu:
>
> $$
> \pi_k^{13} (1-\pi_k)^{87}
> $$
>
> Anschlie√üend normalisieren wir diese Wahrscheinlichkeiten, damit sie sich zu 1 summieren. Die Positionen des h√∂chsten Wertes geben uns den Modus, und wir k√∂nnen die kumulativen Wahrscheinlichkeiten berechnen, um den Median zu finden.
>
> #### Berechnungen
>
> Der **Posteriori-Erwartungswert** f√ºr dieses Gitter kann n√§herungsweise berechnet werden als:
>
> $$
> E[\pi] \approx \sum_{k=0}^{100} \pi_k \cdot P(\pi = \pi_k \mid x=13, n=100)
> $$
>
> Der **Posteriori-Median** wird identifiziert, indem die kumulativen Wahrscheinlichkeiten berechnet werden, bis sie 0.5 erreichen.
>
> F√ºr pr√§zisere numerische Berechnungen, besonders im diskreten Fall, sind Softwaretools wie Python oder R hilfreich, um die Wahrscheinlichkeiten zu berechnen und zu normalisieren.

### $n = 1000, X = 33$

$x \sim B(n, \pi)$, $n=1000$ und $x=33$

## Fall 1: stetige Gleichverteilung von $\pi$ ($\pi \sim U(0,1)$ )

### Priori

$\pi = U(0,1) \longrightarrow \text{Beta Verteilung mit} \space a = 1\space und\space \beta =1\longrightarrow B=(1,1)$ 4

### Likelihood

$$
\pi^{33}(1-\pi)^{967}
$$

### Posteriori

$$
B(1+33,1+1000-33) = B(34,968)
$$

### [[Posteriori-Erwartungswert]]

$$
E[\pi] = \frac{a}{a+b} = \frac{34}{968} \approx0.0351
$$

### Posteriori-Median

## Fall 2: Diskrete Gleichverteilung von $\pi$

### Priori

$\pi \space diskret \space verteilt \space auf \space [0,0.01,0.02,‚Ä¶,0.99,1] \space und \space jeder \space Wert \space gleich \space verteilt$

### Likelihood

$$
\pi^{33}(1-\pi)^{967}
$$

### Posteriori

$$
\pi_{k}^{33}(1-\pi_{k})^{967}
$$

### Posteriori-Erwartungswert

$$
\begin{aligned}
E[\pi] &\approx \sum\limits^{100}_{k=0}\pi_{k}\cdot P(\pi =\pi_{k}|x=33,n=1000)\\
&\approx 0.03382706
\end{aligned}
$$

> [!info]- R-Code zur Berechnung
> #Frage gibt es einen anderen Weg zur Berechnung?
>
> ```r
> # Angenommene Werte
> x <- 33  # Anzahl der Erfolge
> n <- 1000  # Gesamtzahl der Versuche
>
> # Diskrete Werte f√ºr pi
> pi_values <- seq(0, 1, by = 0.01)
>
> # Priori-Wahrscheinlichkeiten (gleich f√ºr alle Werte)
> prior_probs <- rep(1/length(pi_values), length(pi_values))
>
> # Likelihood-Funktion f√ºr jeden Wert von pi berechnen
> likelihoods <- dbinom(x, size = n, prob = pi_values)
>
> # Posteriori-Wahrscheinlichkeiten berechnen
> post_probs <- likelihoods * prior_probs
> post_probs <- post_probs / sum(post_probs)  # Normalisierung
>
> # Posteriori-Erwartungswert berechnen
> posterior_mean <- sum(pi_values * post_probs)
>
> # Ausgabe des Posteriori-Erwartungswertes
> print(posterior_mean)
> ```

### Posteriori-Median

$$
\begin{aligned}
\text{Median}[\pi] &= \pi_{k} \text{, f√ºr das gilt } \sum\limits_{i=0}^{k}P(\pi =\pi_{i}|x=33,n=1000) \geq 0.5\\
&= 0.03
\end{aligned}
$$

Hierbei repr√§sentiert $pi_{k}$ den Wert der diskreten Zufallsvariablen $pi$ an der Stelle $k$ auf dem Gitter, und $P(\pi =\pi_{i}|x=33,n=1000)$ sind die Posteriori-Wahrscheinlichkeiten, kumuliert bis zum Punkt, wo die Summe zum ersten Mal $0,5$ √ºbersteigt, was den Median definiert.

> [!info]- R-Code zur Berechnung
> #Frage gibt es einen anderen Weg zur Berechnung?
>
> ```r
> # Kumulative Posteriori-Wahrscheinlichkeiten berechnen
> cumulative_post_probs <- cumsum(post_probs)
>
> # Finde den Posteriori-Median
> # Dies ist der erste Wert von pi, bei dem die kumulative Wahrscheinlichkeit >= 0.5 ist.
> posterior_median <- pi_values[which(cumulative_post_probs >= 0.5)[1]]
>
> # Ausgabe des Posteriori-Medians
> print(posterior_median)
> ```

## (d) Vergleichen Sie die Posteriori-Erwartungswerte und -Mediane mit beiden Ans√§tzen.

> [!summary] Zusammenfassung der L√∂sung
>
> ### Vergleich von Posteriori-Erwartungswerten und Medianen
>
> Die Analyse f√ºr $n = 1000$ und $x = 33$ ergibt sowohl im stetigen als auch im diskreten Fall √§hnliche Posteriori-Erwartungswerte, mit einem geringf√ºgig h√∂heren Wert im stetigen Fall ($E[\pi] \approx 0.0339$) verglichen mit dem diskreten Fall ($E[\pi] \approx 0.03382706$). Der Posteriori-Median im diskreten Fall ($\text{Median}[\pi] = 0.03$) f√§llt niedriger aus, was typisch ist, da Mediane durch Extremwerte weniger beeinflusst werden. Im stetigen Fall wurde der Median nicht direkt berechnet, w√ºrde aber √§hnlich liegen und leicht unter dem Erwartungswert, aufgrund der Schiefe der Beta-Verteilung.
>
> Diese Ergebnisse illustrieren die Konsistenz der Bayesschen Methodik, da beide Ans√§tze trotz ihrer Unterschiede in Stetigkeit und Diskretheit zu vergleichbaren Schl√ºssen f√ºhren. Die Wahl des Ansatzes sollte auf den spezifischen Kontext der verf√ºgbaren Daten und den gew√ºnschten Detaillierungsgrad der Analyse abgestimmt werden.

> [!success] Ausf√ºhrliche L√∂sung
>
> ### Stetige Gleichverteilung (Fall 1)
>
> F√ºr den stetigen Fall haben wir eine Beta-Verteilung als Priori genommen und aufgrund der beobachteten Daten die Parameter aktualisiert. Wir hatten dabei angenommen, dass $n = 1000$ und $x = 33$. Die Posteriori-Verteilung w√§re dementsprechend eine $\text{Beta}(34, 968)$-Verteilung.
>
> - **Posteriori-Erwartungswert:**
>   $$
>   E[\pi] = \frac{\alpha}{\alpha + \beta} = \frac{34}{34 + 968} = \frac{34}{1002} \approx 0.0339
>   $$
> - **Posteriori-Median:**
>   F√ºr Beta-Verteilungen ist eine geschlossene Form f√ºr den Median nicht einfach zu berechnen, aber der Median einer $\text{Beta}(34, 968)$-Verteilung liegt nahe am Erwartungswert und ist wegen der Schiefe der Verteilung etwas niedriger als der Erwartungswert.
>
> ### Diskrete Gleichverteilung (Fall 2)
>
> Im diskreten Fall haben wir ein Gitter von m√∂glichen Werten f√ºr $\pi$ betrachtet und f√ºr jeden dieser Werte die Posteriori-Wahrscheinlichkeiten berechnet.
>
> - **Posteriori-Erwartungswert:**
>   $$
>   E[\pi] \approx 0.03382706
>   $$
> - **Posteriori-Median:**
>   $$
>   \text{Median}[\pi] = 0.03
>   $$
>
> ### Vergleich der Ans√§tze
>
> - **Erwartungswerte:**
>   Die Posteriori-Erwartungswerte sind sehr √§hnlich, aber nicht identisch, was auf die diskrete Natur des zweiten Ansatzes zur√ºckzuf√ºhren ist.
> - **Mediane:**
>   Der Median im diskreten Fall ist explizit angegeben und etwas niedriger als der Erwartungswert. Im stetigen Fall haben wir den genauen Median nicht berechnet, aber aufgrund der Schiefe der Beta-Verteilung k√∂nnen wir erwarten, dass er ebenfalls etwas niedriger als der Erwartungswert ist.
>
> ### Interpretation
>
> In beiden F√§llen spiegeln die Posteriori-Erwartungswerte und -Mediane die aktualisierte √úberzeugung √ºber die Erfolgswahrscheinlichkeit nach Ber√ºcksichtigung der beobachteten Daten wider. Der Erwartungswert gibt dabei einen zentralen Tendenzpunkt an, w√§hrend der Median eine alternative punktuelle Sch√§tzung ist, die von Extremwerten weniger beeinflusst wird.
>
> Die N√§he der Ergebnisse zeigt, dass beide Ans√§tze zu √§hnlichen Schlussfolgerungen f√ºhren, was die Robustheit der Bayesschen Analyse unterstreicht. In der Praxis w√ºrde die Wahl zwischen den Ans√§tzen von der Art der verf√ºgbaren Informationen und der gew√ºnschten Feinheit der Analyse abh√§ngen.

## (e) Welchen Ansatz w√ºrden Sie eher bevorzugen?

---

# Aufgabe 4

> [!note] Aufgabenstellung
> Betrachten Sie das Poisson-Modell, d.h. $X \sim Po(\lambda)$ und f√ºr den Parameter $\lambda$ wird eine $Ga(\alpha, \beta)$-Priori-Verteilung angenommen.

## (a) Berechnen sie die Posteriori-Verteilung $p(\lambda|X)$ explizit, d.h. inklusive Normierungskonstante.

> [!tip] Hinweis
> $\Gamma(x) = \int_{0}^{\infty} t^{x-1} e^{-t} \, dt$.

Die Posteriori-Verteilung ergibt sich aus dem Produkt der Likelihood-Funktion des Poisson-Modells und der Gamma-Priori-Verteilung. F√ºr eine gegebene Anzahl von Ereignissen $x$ ist die Likelihood $L(\lambda) = \frac{e^{-\lambda} \lambda^x}{x!}$. Die Priori-Verteilung ist gegeben durch die Dichtefunktion der Gamma-Verteilung $f(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}$.

Das Produkt aus Likelihood und Priori ergibt die nicht normierte Posteriori-Verteilung:

$$
p(\lambda|X) \propto \lambda^{\alpha + x - 1} e^{-\lambda(\beta + 1)}
$$

Um die Normierungskonstante zu bestimmen, nutzen wir die Definition der Gamma-Funktion:

$$
\Gamma(\alpha + x) = \int_{0}^{\infty} t^{\alpha + x - 1} e^{-t} \, dt
$$

Die Posteriori-Verteilung, inklusive der Normierungskonstante, ist daher eine Gamma-Verteilung $Ga(\alpha + x, \beta + 1)$:

$$
p(\lambda|X) = \frac{(\beta + 1)^{\alpha + x}}{\Gamma(\alpha + x)} \lambda^{\alpha + x - 1} e^{-(\beta + 1)\lambda}
$$

## (b) Warum gen√ºgt es, die Posteriori nur bis auf eine multiplikative Konstante zu bestimmen?

Es gen√ºgt, die Posteriori nur bis auf eine multiplikative Konstante zu bestimmen, weil wir meistens an den relativen Wahrscheinlichkeiten von $\lambda$ interessiert sind und nicht an den absoluten Wahrscheinlichkeiten. F√ºr die meisten bayesianischen Inferenzprobleme, wie die Berechnung von Erwartungswerten oder die Bestimmung von Konfidenzintervallen, k√ºrzt sich die Konstante heraus. Au√üerdem kann die Konstante oft komplex sein und ihre explizite Berechnung kann unn√∂tig aufw√§ndig sein, insbesondere wenn nur der Posteriori-Modus oder -Median und nicht die vollst√§ndige Posteriori-Verteilung von Interesse ist.

---

# Aufgabe 4

> [!note] Aufgabenstellung
> Betrachten Sie das Poisson-Modell, d.h. $X \sim Po(\lambda)$ und f√ºr den Parameter $\lambda$ wird eine $Ga(\alpha, \beta)$-Priori-Verteilung angenommen.

## (a) Berechnen sie die Posteriori-Verteilung $p(\lambda|X)$ explizit, d.h. inklusive Normierungskonstante.

> [!tip] Hinweis
> $\Gamma(x) = \int_{0}^{\infty} t^{x-1} e^{-t} \, dt$.

## (b) Warum gen√ºgt es, die Posteriori nur bis auf eine multiplikative Konstante zu bestimmen?

<!-- DISQUS SCRIPT COMMENT START -->

<hr style="border: none; height: 2px; background: linear-gradient(to right, #f0f0f0, #ccc, #f0f0f0); margin-top: 4rem; margin-bottom: 5rem;">
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://myuninotes.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<!-- DISQUS SCRIPT COMMENT END -->
